{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JUJoyVOtYPwm"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        ".appName('DAG and Broadcast demo')\\\n",
        ".getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orders_data = [\n",
        "    (\"O001\",\"Hyderabad\",1200),\n",
        "    (\"O002\",\"Delhi\",800),\n",
        "    (\"O003\",\"Mumbai\",1500),\n",
        "    (\"O004\",\"Bangalore\",400),\n",
        "    (\"O005\",\"Hyderabad\",300),\n",
        "    (\"O006\",\"Delhi\",2000),\n",
        "    (\"O007\",\"Mumbai\",700),\n",
        "    (\"O008\",\"Bangalore\",1800),\n",
        "    (\"O009\",\"Delhi\",350),\n",
        "    (\"O010\",\"Hyderabad\",900)\n",
        "]\n",
        "orders_cols = [\"order_id\",\"city\",\"order_amount\"]\n",
        "orders_df = spark.createDataFrame(orders_data,orders_cols)\n",
        "orders_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMdSebkvYdtW",
        "outputId": "550f6827-92fc-4577-c48c-40578a31bcf9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+------------+\n",
            "|order_id|     city|order_amount|\n",
            "+--------+---------+------------+\n",
            "|    O001|Hyderabad|        1200|\n",
            "|    O002|    Delhi|         800|\n",
            "|    O003|   Mumbai|        1500|\n",
            "|    O004|Bangalore|         400|\n",
            "|    O005|Hyderabad|         300|\n",
            "|    O006|    Delhi|        2000|\n",
            "|    O007|   Mumbai|         700|\n",
            "|    O008|Bangalore|        1800|\n",
            "|    O009|    Delhi|         350|\n",
            "|    O010|Hyderabad|         900|\n",
            "+--------+---------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "city_data = [\n",
        "    (\"Hyderabad\",\"Tier 1\"),\n",
        "    (\"Delhi\",\"Tier 1\"),\n",
        "    (\"Mumbai\",\"Tier 1\"),\n",
        "    (\"Bangalore\",\"Tier 1\")\n",
        "]\n",
        "city_cols = [\"city\",\"category\"]\n",
        "city_df = spark.createDataFrame(city_data,city_cols)\n",
        "city_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC6vnym3Yx15",
        "outputId": "34857d83-75ff-4ce3-c75e-735c47771a7b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+\n",
            "|     city|category|\n",
            "+---------+--------+\n",
            "|Hyderabad|  Tier 1|\n",
            "|    Delhi|  Tier 1|\n",
            "|   Mumbai|  Tier 1|\n",
            "|Bangalore|  Tier 1|\n",
            "+---------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "filtered_orders = orders_df.filter(col(\"order_amount\") > 500)"
      ],
      "metadata": {
        "id": "lcII9siXZZs2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joined_df = filtered_orders.join(\n",
        "    city_df,\n",
        "    on=\"city\",\n",
        "    how=\"inner\"\n",
        ")"
      ],
      "metadata": {
        "id": "mAIE0yW9ZRFN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = joined_df.select(\n",
        "    \"order_id\",\n",
        "    \"city\",\n",
        "    \"category\",\n",
        "    \"order_amount\"\n",
        ")"
      ],
      "metadata": {
        "id": "Tzctq7rtZuNv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrnRy4_dZw3d",
        "outputId": "ee762002-06a8-4283-e0ce-fc01c93d9b54"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Project ['order_id, 'city, 'category, 'order_amount]\n",
            "+- Project [city#23, order_id#22, order_amount#24L, category#36]\n",
            "   +- Join Inner, (city#23 = city#35)\n",
            "      :- Filter (order_amount#24L > cast(500 as bigint))\n",
            "      :  +- LogicalRDD [order_id#22, city#23, order_amount#24L], false\n",
            "      +- LogicalRDD [city#35, category#36], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "order_id: string, city: string, category: string, order_amount: bigint\n",
            "Project [order_id#22, city#23, category#36, order_amount#24L]\n",
            "+- Project [city#23, order_id#22, order_amount#24L, category#36]\n",
            "   +- Join Inner, (city#23 = city#35)\n",
            "      :- Filter (order_amount#24L > cast(500 as bigint))\n",
            "      :  +- LogicalRDD [order_id#22, city#23, order_amount#24L], false\n",
            "      +- LogicalRDD [city#35, category#36], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [order_id#22, city#23, category#36, order_amount#24L]\n",
            "+- Join Inner, (city#23 = city#35)\n",
            "   :- Filter ((isnotnull(order_amount#24L) AND (order_amount#24L > 500)) AND isnotnull(city#23))\n",
            "   :  +- LogicalRDD [order_id#22, city#23, order_amount#24L], false\n",
            "   +- Filter isnotnull(city#35)\n",
            "      +- LogicalRDD [city#35, category#36], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [order_id#22, city#23, category#36, order_amount#24L]\n",
            "   +- SortMergeJoin [city#23], [city#35], Inner\n",
            "      :- Sort [city#23 ASC NULLS FIRST], false, 0\n",
            "      :  +- Exchange hashpartitioning(city#23, 200), ENSURE_REQUIREMENTS, [plan_id=73]\n",
            "      :     +- Filter ((isnotnull(order_amount#24L) AND (order_amount#24L > 500)) AND isnotnull(city#23))\n",
            "      :        +- Scan ExistingRDD[order_id#22,city#23,order_amount#24L]\n",
            "      +- Sort [city#35 ASC NULLS FIRST], false, 0\n",
            "         +- Exchange hashpartitioning(city#35, 200), ENSURE_REQUIREMENTS, [plan_id=74]\n",
            "            +- Filter isnotnull(city#35)\n",
            "               +- Scan ExistingRDD[city#35,category#36]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import broadcast\n",
        "broadcast_join_df = filtered_orders.join(\n",
        "    broadcast(city_df),\n",
        "    on=\"city\",\n",
        "    how=\"inner\"\n",
        ")\n",
        "\n",
        "final_broadcast_df = broadcast_join_df.select(\n",
        "    \"order_id\",\n",
        "    \"city\",\n",
        "    \"category\",\n",
        "    \"order_amount\"\n",
        ")"
      ],
      "metadata": {
        "id": "lgHpPKDiaruL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_broadcast_df.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-m0Loh76a8OZ",
        "outputId": "14730f49-e8c7-4699-a56e-067e0b301240"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Project ['order_id, 'city, 'category, 'order_amount]\n",
            "+- Project [city#23, order_id#22, order_amount#24L, category#36]\n",
            "   +- Join Inner, (city#23 = city#35)\n",
            "      :- Filter (order_amount#24L > cast(500 as bigint))\n",
            "      :  +- LogicalRDD [order_id#22, city#23, order_amount#24L], false\n",
            "      +- ResolvedHint (strategy=broadcast)\n",
            "         +- LogicalRDD [city#35, category#36], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "order_id: string, city: string, category: string, order_amount: bigint\n",
            "Project [order_id#22, city#23, category#36, order_amount#24L]\n",
            "+- Project [city#23, order_id#22, order_amount#24L, category#36]\n",
            "   +- Join Inner, (city#23 = city#35)\n",
            "      :- Filter (order_amount#24L > cast(500 as bigint))\n",
            "      :  +- LogicalRDD [order_id#22, city#23, order_amount#24L], false\n",
            "      +- ResolvedHint (strategy=broadcast)\n",
            "         +- LogicalRDD [city#35, category#36], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [order_id#22, city#23, category#36, order_amount#24L]\n",
            "+- Join Inner, (city#23 = city#35), rightHint=(strategy=broadcast)\n",
            "   :- Filter ((isnotnull(order_amount#24L) AND (order_amount#24L > 500)) AND isnotnull(city#23))\n",
            "   :  +- LogicalRDD [order_id#22, city#23, order_amount#24L], false\n",
            "   +- Filter isnotnull(city#35)\n",
            "      +- LogicalRDD [city#35, category#36], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=true\n",
            "+- == Final Plan ==\n",
            "   ResultQueryStage 1\n",
            "   +- *(2) Project [order_id#22, city#23, category#36, order_amount#24L]\n",
            "      +- *(2) BroadcastHashJoin [city#23], [city#35], Inner, BuildRight, false\n",
            "         :- *(2) Filter ((isnotnull(order_amount#24L) AND (order_amount#24L > 500)) AND isnotnull(city#23))\n",
            "         :  +- *(2) Scan ExistingRDD[order_id#22,city#23,order_amount#24L]\n",
            "         +- BroadcastQueryStage 0\n",
            "            +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=112]\n",
            "               +- *(1) Filter isnotnull(city#35)\n",
            "                  +- *(1) Scan ExistingRDD[city#35,category#36]\n",
            "+- == Initial Plan ==\n",
            "   Project [order_id#22, city#23, category#36, order_amount#24L]\n",
            "   +- BroadcastHashJoin [city#23], [city#35], Inner, BuildRight, false\n",
            "      :- Filter ((isnotnull(order_amount#24L) AND (order_amount#24L > 500)) AND isnotnull(city#23))\n",
            "      :  +- Scan ExistingRDD[order_id#22,city#23,order_amount#24L]\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=103]\n",
            "         +- Filter isnotnull(city#35)\n",
            "            +- Scan ExistingRDD[city#35,category#36]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Broadcast Hash Join\n",
        "\n",
        "Purpose: Optimizes Spark joins between a large DataFrame and a small DataFrame.\n",
        "\n",
        "Mechanism: The small DataFrame is copied (broadcasted) to all worker nodes in the cluster.\n",
        "Execution: Each worker node then performs a local hash join between its partitions of the large DataFrame and the broadcasted small DataFrame.\n",
        "\n",
        "Benefit 1 (Shuffle): Eliminates costly shuffle operations on the large DataFrame, which would otherwise involve network I/O.\n",
        "\n",
        "Benefit 2 (Speed): Leads to faster joins because local hash joins are quicker than shuffle-based joins.\n",
        "\n",
        "Benefit 3 (Efficiency): Improves resource efficiency by reducing network traffic and disk I/O.\n",
        "\n",
        "Indication: In Spark's physical plan (.explain(True)), it's identified as BroadcastHashJoin with a BroadcastExchange step.\n"
      ],
      "metadata": {
        "id": "-r6GOQptckZv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "foAEt4R2bA7r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}