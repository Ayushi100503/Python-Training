{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8qPe-5BDgX_n"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        ".appName('DAG and Broadcast')\\\n",
        ".getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rides_data = [\n",
        "(\"R001\",\"U001\",\"Hyderabad\",12.5,240,\"Completed\"),\n",
        "(\"R002\",\"U002\",\"Delhi\",8.2,180,\"Completed\"),\n",
        "(\"R003\",\"U003\",\"Mumbai\",15.0,300,\"Cancelled\"),\n",
        "(\"R004\",\"U004\",\"Bangalore\",5.5,120,\"Completed\"),\n",
        "(\"R005\",\"U005\",\"Hyderabad\",20.0,360,\"Completed\"),\n",
        "(\"R006\",\"U006\",\"Delhi\",25.0,420,\"Completed\"),\n",
        "(\"R007\",\"U007\",\"Mumbai\",7.5,150,\"Completed\"),\n",
        "(\"R008\",\"U008\",\"Bangalore\",18.0,330,\"Completed\"),\n",
        "(\"R009\",\"U009\",\"Delhi\",6.0,140,\"Cancelled\"),\n",
        "(\"R010\",\"U010\",\"Hyderabad\",10.0,200,\"Completed\")\n",
        "]\n",
        "rides_cols = [\n",
        "\"ride_id\",\n",
        "\"user_id\",\n",
        "\"city\",\n",
        "\"distance_km\",\n",
        "\"duration_seconds\",\n",
        "\"status\"\n",
        "]\n",
        "\n",
        "rides_df = spark.createDataFrame(rides_data, rides_cols)"
      ],
      "metadata": {
        "id": "E2DJxgl8gcT-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "surge_data = [\n",
        "(\"Hyderabad\",1.2),\n",
        "(\"Delhi\",1.5),\n",
        "(\"Mumbai\",1.8),\n",
        "(\"Bangalore\",1.3)\n",
        "]\n",
        "surge_cols = [\"city\",\"surge_multiplier\"]\n",
        "surge_df = spark.createDataFrame(surge_data, surge_cols)"
      ],
      "metadata": {
        "id": "kSKXPamRgkVq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EXERCISE SET 1 — TRANSFORMATIONS vs ACTIONS"
      ],
      "metadata": {
        "id": "7F6J_AijhtI0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 1.1\n",
        "\n",
        "Create a transformation pipeline that:\n",
        "Filters only Completed rides\n",
        "Selects ride_id , city , distance_km\n",
        "\n",
        "Tasks:\n",
        "Do not trigger any action\n",
        "\n",
        "Explain whether Spark executed anything: no it did not execute anything"
      ],
      "metadata": {
        "id": "aAgdCF5ihMjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "completed_rides_df = (\n",
        "    rides_df\n",
        "    .filter(\"status = 'Completed'\")\n",
        "    .select(\"ride_id\", \"city\", \"distance_km\")\n",
        ")"
      ],
      "metadata": {
        "id": "NF0AXmGFgotv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 1.2\n",
        "\n",
        "Trigger a single action on the pipeline. Tasks:\n",
        "\n",
        "Identify which line caused execution: .show() Explain why previous lines did not execute. it does not execute till an action is triggered."
      ],
      "metadata": {
        "id": "RDEHTQ8rhneX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "completed_rides_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sda3t7rDhDIr",
        "outputId": "cc25a4d2-610a-4dfd-fa9f-258d4bdfdc31"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+-----------+\n",
            "|ride_id|     city|distance_km|\n",
            "+-------+---------+-----------+\n",
            "|   R001|Hyderabad|       12.5|\n",
            "|   R002|    Delhi|        8.2|\n",
            "|   R004|Bangalore|        5.5|\n",
            "|   R005|Hyderabad|       20.0|\n",
            "|   R006|    Delhi|       25.0|\n",
            "|   R007|   Mumbai|        7.5|\n",
            "|   R008|Bangalore|       18.0|\n",
            "|   R010|Hyderabad|       10.0|\n",
            "+-------+---------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EXERCISE SET 2 — DAG & LINEAGE"
      ],
      "metadata": {
        "id": "gfd0sCYAhY4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 2.1\n",
        "\n",
        "Create a transformation chain with:\n",
        "Multiple filters\n",
        "A column selection\n",
        "Tasks:\n",
        "Run explain(True)\n",
        "Identify:\n",
        "Logical plan\n",
        "Optimized logical plan\n",
        "Physical plan"
      ],
      "metadata": {
        "id": "vaA4Rxtdh6-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_chain = (\n",
        "    rides_df\n",
        "    .filter(\"status = 'Completed'\")\n",
        "    .filter(\"distance_km > 10\")\n",
        "    .select(\"ride_id\", \"city\", \"distance_km\")\n",
        ")\n",
        "df_chain.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ca3wgpLHhJhT",
        "outputId": "4d95fc8f-43fd-424c-b99e-6d3a0b7cbe4e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Project ['ride_id, 'city, 'distance_km]\n",
            "+- Filter (distance_km#3 > cast(10 as double))\n",
            "   +- Filter (status#5 = Completed)\n",
            "      +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "ride_id: string, city: string, distance_km: double\n",
            "Project [ride_id#0, city#2, distance_km#3]\n",
            "+- Filter (distance_km#3 > cast(10 as double))\n",
            "   +- Filter (status#5 = Completed)\n",
            "      +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [ride_id#0, city#2, distance_km#3]\n",
            "+- Filter ((isnotnull(status#5) AND isnotnull(distance_km#3)) AND ((status#5 = Completed) AND (distance_km#3 > 10.0)))\n",
            "   +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [ride_id#0, city#2, distance_km#3]\n",
            "+- *(1) Filter ((isnotnull(status#5) AND isnotnull(distance_km#3)) AND ((status#5 = Completed) AND (distance_km#3 > 10.0)))\n",
            "   +- *(1) Scan ExistingRDD[ride_id#0,user_id#1,city#2,distance_km#3,duration_seconds#4L,status#5]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 2.2\n",
        "\n",
        "Reorder transformations (filter after join vs before join).\n",
        "Tasks:\n",
        "Compare DAGs\n",
        "Identify which plan is more efficient and why"
      ],
      "metadata": {
        "id": "qvhr1QThihEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#bad plan\n",
        "bad_plan = rides_df.join(surge_df, \"city\").filter(\"distance_km > 10\")\n",
        "bad_plan.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4AowfihiMWO",
        "outputId": "1a342e21-154d-4a81-c214-dbe6166f7a68"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Filter ('distance_km > 10)\n",
            "+- Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "   +- Join Inner, (city#2 = city#6)\n",
            "      :- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "      +- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, ride_id: string, user_id: string, distance_km: double, duration_seconds: bigint, status: string, surge_multiplier: double\n",
            "Filter (distance_km#3 > cast(10 as double))\n",
            "+- Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "   +- Join Inner, (city#2 = city#6)\n",
            "      :- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "      +- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "+- Join Inner, (city#2 = city#6)\n",
            "   :- Filter ((isnotnull(distance_km#3) AND (distance_km#3 > 10.0)) AND isnotnull(city#2))\n",
            "   :  +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "   +- Filter isnotnull(city#6)\n",
            "      +- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "   +- SortMergeJoin [city#2], [city#6], Inner\n",
            "      :- Sort [city#2 ASC NULLS FIRST], false, 0\n",
            "      :  +- Exchange hashpartitioning(city#2, 200), ENSURE_REQUIREMENTS, [plan_id=57]\n",
            "      :     +- Filter ((isnotnull(distance_km#3) AND (distance_km#3 > 10.0)) AND isnotnull(city#2))\n",
            "      :        +- Scan ExistingRDD[ride_id#0,user_id#1,city#2,distance_km#3,duration_seconds#4L,status#5]\n",
            "      +- Sort [city#6 ASC NULLS FIRST], false, 0\n",
            "         +- Exchange hashpartitioning(city#6, 200), ENSURE_REQUIREMENTS, [plan_id=58]\n",
            "            +- Filter isnotnull(city#6)\n",
            "               +- Scan ExistingRDD[city#6,surge_multiplier#7]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#good plan\n",
        "good_plan = rides_df.filter(\"distance_km > 10\").join(surge_df, \"city\")\n",
        "good_plan.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLrx7D1bjCf0",
        "outputId": "6ad38363-270f-4f28-9132-cf5789a275cc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(Inner, [city])\n",
            ":- Filter (distance_km#3 > cast(10 as double))\n",
            ":  +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "+- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, ride_id: string, user_id: string, distance_km: double, duration_seconds: bigint, status: string, surge_multiplier: double\n",
            "Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "+- Join Inner, (city#2 = city#6)\n",
            "   :- Filter (distance_km#3 > cast(10 as double))\n",
            "   :  +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "   +- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "+- Join Inner, (city#2 = city#6)\n",
            "   :- Filter ((isnotnull(distance_km#3) AND (distance_km#3 > 10.0)) AND isnotnull(city#2))\n",
            "   :  +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "   +- Filter isnotnull(city#6)\n",
            "      +- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "   +- SortMergeJoin [city#2], [city#6], Inner\n",
            "      :- Sort [city#2 ASC NULLS FIRST], false, 0\n",
            "      :  +- Exchange hashpartitioning(city#2, 200), ENSURE_REQUIREMENTS, [plan_id=88]\n",
            "      :     +- Filter ((isnotnull(distance_km#3) AND (distance_km#3 > 10.0)) AND isnotnull(city#2))\n",
            "      :        +- Scan ExistingRDD[ride_id#0,user_id#1,city#2,distance_km#3,duration_seconds#4L,status#5]\n",
            "      +- Sort [city#6 ASC NULLS FIRST], false, 0\n",
            "         +- Exchange hashpartitioning(city#6, 200), ENSURE_REQUIREMENTS, [plan_id=89]\n",
            "            +- Filter isnotnull(city#6)\n",
            "               +- Scan ExistingRDD[city#6,surge_multiplier#7]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "filter before join is better as it reduces data before shuffle, smaller join input, less network io, fewer cpu cycles"
      ],
      "metadata": {
        "id": "sXyauQh8jTIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EXERCISE SET 3 — PARTITIONS & SHUFFLE"
      ],
      "metadata": {
        "id": "ZrqxAFhcjmD6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 3.1\n",
        "\n",
        "Check the number of partitions of rides_df .\n",
        "Tasks:\n",
        "Repartition into 4 partitions\n",
        "\n",
        "Coalesce into 1 partition\n",
        "Observe number of output files when writing to Parquet"
      ],
      "metadata": {
        "id": "6YtarhTcjnXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rides_df.rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kljNUZp3jSCA",
        "outputId": "eb21d55a-fd26-4bf6-ab9a-5a2625650202"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rides_4 = rides_df.repartition(4)\n",
        "rides_4.rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wizu5ycckGTP",
        "outputId": "9957742b-7ea9-4881-8788-1beddbd5661d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rides_1 = rides_df.coalesce(1)\n",
        "rides_1.rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdWmjDlzkgTW",
        "outputId": "9065978b-4c0a-4227-d3d2-3d26a9fe984f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rides_4.write.mode(\"overwrite\").parquet(\"out_4\")\n",
        "rides_1.write.mode(\"overwrite\").parquet(\"out_1\")"
      ],
      "metadata": {
        "id": "-MkMEKlNlMUT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 3.2\n",
        "\n",
        "Repartition rides by city .\n",
        "Tasks:\n",
        "Run explain(True)\n",
        "Identify whether a shuffle is introduced"
      ],
      "metadata": {
        "id": "cxqiYXfeldTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rides_city = rides_df.repartition(\"city\")\n",
        "rides_city.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeTm6P4XlZlN",
        "outputId": "e489d062-caaf-4f27-8cbf-8d2cc9bff8f1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'RepartitionByExpression ['city]\n",
            "+- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "ride_id: string, user_id: string, city: string, distance_km: double, duration_seconds: bigint, status: string\n",
            "RepartitionByExpression [city#2]\n",
            "+- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "RepartitionByExpression [city#2]\n",
            "+- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Exchange hashpartitioning(city#2, 200), REPARTITION_BY_COL, [plan_id=182]\n",
            "   +- Scan ExistingRDD[ride_id#0,user_id#1,city#2,distance_km#3,duration_seconds#4L,status#5]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shuffle is introduced as data redistributed based on city, requires exchange"
      ],
      "metadata": {
        "id": "_L6jm-dnlofe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EXERCISE SET 4 — JOIN WITHOUT BROADCAST (BAD DAG)"
      ],
      "metadata": {
        "id": "UhEysOYbmDir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 4.1\n",
        "\n",
        "Join rides_df with surge_df on city without using broadcast.\n",
        "Tasks:\n",
        "Run explain(True)\n",
        "Identify:\n",
        "Join type\n",
        "Exchange operators\n",
        "Sort operations\n",
        "Stage boundaries"
      ],
      "metadata": {
        "id": "ekvN_85CmGQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "join_df = rides_df.join(surge_df, \"city\")\n",
        "join_df.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qybHr4v4lmFN",
        "outputId": "ba12c5e9-499d-4796-ccd5-4c49386d33ec"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(Inner, [city])\n",
            ":- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "+- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, ride_id: string, user_id: string, distance_km: double, duration_seconds: bigint, status: string, surge_multiplier: double\n",
            "Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "+- Join Inner, (city#2 = city#6)\n",
            "   :- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "   +- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "+- Join Inner, (city#2 = city#6)\n",
            "   :- Filter isnotnull(city#2)\n",
            "   :  +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "   +- Filter isnotnull(city#6)\n",
            "      +- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "   +- SortMergeJoin [city#2], [city#6], Inner\n",
            "      :- Sort [city#2 ASC NULLS FIRST], false, 0\n",
            "      :  +- Exchange hashpartitioning(city#2, 200), ENSURE_REQUIREMENTS, [plan_id=209]\n",
            "      :     +- Filter isnotnull(city#2)\n",
            "      :        +- Scan ExistingRDD[ride_id#0,user_id#1,city#2,distance_km#3,duration_seconds#4L,status#5]\n",
            "      +- Sort [city#6 ASC NULLS FIRST], false, 0\n",
            "         +- Exchange hashpartitioning(city#6, 200), ENSURE_REQUIREMENTS, [plan_id=210]\n",
            "            +- Filter isnotnull(city#6)\n",
            "               +- Scan ExistingRDD[city#6,surge_multiplier#7]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 4.2\n",
        "\n",
        "Apply a filter ( distance_km > 10 ) before the join.\n",
        "Tasks:\n",
        "Observe whether shuffle is removed\n",
        "Explain why or why not"
      ],
      "metadata": {
        "id": "VXP3ciw-mYLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_join = rides_df.filter(\"distance_km > 10\").join(surge_df, \"city\")\n",
        "filtered_join.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUKs8udImWNb",
        "outputId": "3e656146-9a3b-4e16-d29c-c10d2cdf5bd4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(Inner, [city])\n",
            ":- Filter (distance_km#3 > cast(10 as double))\n",
            ":  +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "+- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, ride_id: string, user_id: string, distance_km: double, duration_seconds: bigint, status: string, surge_multiplier: double\n",
            "Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "+- Join Inner, (city#2 = city#6)\n",
            "   :- Filter (distance_km#3 > cast(10 as double))\n",
            "   :  +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "   +- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "+- Join Inner, (city#2 = city#6)\n",
            "   :- Filter ((isnotnull(distance_km#3) AND (distance_km#3 > 10.0)) AND isnotnull(city#2))\n",
            "   :  +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "   +- Filter isnotnull(city#6)\n",
            "      +- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "   +- SortMergeJoin [city#2], [city#6], Inner\n",
            "      :- Sort [city#2 ASC NULLS FIRST], false, 0\n",
            "      :  +- Exchange hashpartitioning(city#2, 200), ENSURE_REQUIREMENTS, [plan_id=240]\n",
            "      :     +- Filter ((isnotnull(distance_km#3) AND (distance_km#3 > 10.0)) AND isnotnull(city#2))\n",
            "      :        +- Scan ExistingRDD[ride_id#0,user_id#1,city#2,distance_km#3,duration_seconds#4L,status#5]\n",
            "      +- Sort [city#6 ASC NULLS FIRST], false, 0\n",
            "         +- Exchange hashpartitioning(city#6, 200), ENSURE_REQUIREMENTS, [plan_id=241]\n",
            "            +- Filter isnotnull(city#6)\n",
            "               +- Scan ExistingRDD[city#6,surge_multiplier#7]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EXERCISE SET 5 — BROADCAST JOIN (GOOD DAG)"
      ],
      "metadata": {
        "id": "VFKvt3l9mlT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 5.1\n",
        "\n",
        "Apply a broadcast hint to surge_df .\n",
        "Tasks:\n",
        "Run explain(True)\n",
        "Identify:\n",
        "Join type\n",
        "BroadcastExchange\n",
        "Disappearance of shuffles"
      ],
      "metadata": {
        "id": "IHm7sn3Rmoxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import broadcast\n",
        "broadcast_join = rides_df.join(\n",
        "    broadcast(surge_df),\n",
        "    \"city\"\n",
        ")\n",
        "broadcast_join.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnIQPKxImjTL",
        "outputId": "89a743b1-09c0-45a3-cd71-53efb5c6b40b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(Inner, [city])\n",
            ":- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, ride_id: string, user_id: string, distance_km: double, duration_seconds: bigint, status: string, surge_multiplier: double\n",
            "Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "+- Join Inner, (city#2 = city#6)\n",
            "   :- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "   +- ResolvedHint (strategy=broadcast)\n",
            "      +- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "+- Join Inner, (city#2 = city#6), rightHint=(strategy=broadcast)\n",
            "   :- Filter isnotnull(city#2)\n",
            "   :  +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "   +- Filter isnotnull(city#6)\n",
            "      +- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "   +- BroadcastHashJoin [city#2], [city#6], Inner, BuildRight, false\n",
            "      :- Filter isnotnull(city#2)\n",
            "      :  +- Scan ExistingRDD[ride_id#0,user_id#1,city#2,distance_km#3,duration_seconds#4L,status#5]\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=270]\n",
            "         +- Filter isnotnull(city#6)\n",
            "            +- Scan ExistingRDD[city#6,surge_multiplier#7]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 5.2\n",
        "\n",
        "Compare physical plans from:\n",
        "Exercise 4.1\n",
        "Exercise 5.1\n",
        "Tasks:\n",
        "List operators that disappeared\n",
        "Explain performance impact"
      ],
      "metadata": {
        "id": "mKzjOApcnH3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Operators that disappeared with Broadcast Join (Exercise 5.1 vs. Exercise 4.1):\n",
        "\n",
        "When you introduced the broadcast hint in Exercise 5.1, the following operators disappeared from the physical plan:\n",
        "\n",
        "Sort operations on both rides_df and surge_df: In SortMergeJoin (Exercise 4.1), both DataFrames needed to be sorted by the join key (city) before the merge could happen. The BroadcastHashJoin (Exercise 5.1) eliminates this need.\n",
        "Exchange hashpartitioning for rides_df: In SortMergeJoin, both DataFrames had to be shuffled (repartitioned) by the join key so that rows with the same key would reside on the same partition across the cluster. With BroadcastHashJoin, only the smaller surge_df is broadcasted, so the larger rides_df does not need to be shuffled.\n",
        "Performance Impact:\n",
        "\n",
        "The disappearance of these operators, particularly the Sort and Exchange (shuffles) for the larger rides_df, has a significant positive performance impact:\n",
        "\n",
        "Reduced Network I/O: Shuffling data across the network is one of the most expensive operations in Spark. BroadcastHashJoin avoids the shuffle of the larger DataFrame entirely, drastically reducing network traffic.\n",
        "Reduced CPU Overhead: Sorting large datasets is CPU-intensive. BroadcastHashJoin removes this sorting step.\n",
        "Faster Join Execution: By eliminating these costly operations, BroadcastHashJoin can be orders of magnitude faster than SortMergeJoin when the broadcasted table is small enough to fit comfortably into the memory of each executor. The BroadcastExchange for surge_df is still a network operation, but it's a one-time transfer of a small dataset to all executors, which is much more efficient than repeatedly shuffling a large dataset.\n"
      ],
      "metadata": {
        "id": "ClVNE-4xoOk7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EXERCISE SET 6 — DAG INTERPRETATION"
      ],
      "metadata": {
        "id": "pOZtB2yQnLvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 6.1\n",
        "\n",
        "From the physical plan:\n",
        "Identify all expensive operators\n",
        "Classify them as CPU, memory, or network heavy"
      ],
      "metadata": {
        "id": "sqodSJM_nPGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exchange hashpartitioning (Shuffle):\n",
        "\n",
        "Classification: Primarily Network-heavy, also CPU-heavy and can be Memory/Disk I/O-heavy.\n",
        "Explanation: This operator is responsible for redistributing data across the cluster, typically when data needs to be grouped, joined, or sorted by a key. It involves serializing data, transferring it over the network, and then deserializing it on the receiving nodes. This is often the most expensive operation in Spark due to the high network overhead and potential disk spills if data doesn't fit in memory.\n",
        "Sort:\n",
        "\n",
        "Classification: Primarily CPU-heavy and Memory-heavy, potentially Disk I/O-heavy.\n",
        "Explanation: Sorting data requires significant computational effort. If the data to be sorted exceeds the available memory on an executor, Spark will spill the data to disk, leading to expensive disk I/O.\n",
        "SortMergeJoin:\n",
        "\n",
        "Classification: Network-heavy, CPU-heavy, Memory-heavy.\n",
        "Explanation: This join strategy combines the costs of shuffling (via Exchange hashpartitioning) and sorting (Sort) on both DataFrames before the merge phase. The sorting and merging process itself also consumes CPU and memory.\n",
        "BroadcastExchange (part of BroadcastHashJoin):\n",
        "\n",
        "Classification: Primarily Network-heavy and Memory-heavy.\n",
        "Explanation: While efficient for small tables, broadcasting involves sending the entire smaller DataFrame to all executor nodes. This is a network operation, and each executor must store the broadcasted data in memory.\n",
        "BroadcastHashJoin:\n",
        "\n",
        "Classification: CPU-heavy and Memory-heavy (after the BroadcastExchange).\n",
        "Explanation: After the broadcast, the smaller DataFrame is converted into a hash table in memory on each executor. The larger DataFrame is then scanned, and its keys are used to probe this in-memory hash table. Hash table creation and probing are CPU-intensive and require memory for the hash table itself.\n",
        "Filter: (Less expensive than shuffles/joins, but can still be significant)\n",
        "\n",
        "Classification: Primarily CPU-heavy.\n",
        "Explanation: Applying filter conditions involves iterating through rows and evaluating predicates. For very large datasets or complex conditions, this can consume"
      ],
      "metadata": {
        "id": "3mfqKMpWojdt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 6.2\n",
        "\n",
        "Explain why Spark defaults to SortMergeJoin ."
      ],
      "metadata": {
        "id": "QD8UtYISnZwu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reliability and Scalability: SortMergeJoin is a more robust and generally scalable join algorithm. It works efficiently regardless of the size of the DataFrames being joined. BroadcastHashJoin, while fast for small tables, can fail if the table to be broadcasted is too large to fit into the memory of a single executor.\n",
        "\n",
        "No Data Size Assumption: Spark's optimizer doesn't always know the exact size of the DataFrames at the planning stage, especially if they are derived from complex transformations. SortMergeJoin doesn't require prior knowledge about data size to guarantee execution, making it a safe default.\n",
        "\n",
        "Memory Constraints: Broadcating a large table can exhaust the memory of executors, leading to out-of-memory errors. SortMergeJoin handles larger datasets by spilling data to disk if memory is insufficient during the sort phase, which is slower but prevents crashes.\n",
        "\n",
        "No Broadcast Limit: By default, Spark has a configuration (spark.sql.autoBroadcastJoinThreshold) that sets a size limit for auto-broadcasting. If a DataFrame is smaller than this threshold, Spark might automatically use BroadcastHashJoin even without an explicit hint. However, if it exceeds this threshold (or if the threshold is not met for other reasons), SortMergeJoin is the fallback.\n",
        "\n",
        "Handling Skewed Data: SortMergeJoin can handle skewed data distributions more gracefully than HashJoin (which BroadcastHashJoin is a variant of). In HashJoin, if a key has many values, the single task processing that key can become a bottleneck. SortMergeJoin distributes the work more evenly after the initial sort."
      ],
      "metadata": {
        "id": "rX3_iN-Qovgb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EXERCISE SET 7 — ACTION-DRIVEN EXECUTION"
      ],
      "metadata": {
        "id": "lh03dtJ9neFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 7.1\n",
        "\n",
        "Create a long transformation pipeline without any action.\n",
        "Tasks:\n",
        "Explain what Spark has done so far"
      ],
      "metadata": {
        "id": "UOQY_tEWnhl6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 7.2\n",
        "\n",
        "Trigger different actions ( count , show , write ) separately.\n",
        "Tasks:\n",
        "Observe whether Spark recomputes the DAG\n",
        "Explain behavior"
      ],
      "metadata": {
        "id": "HabCAVDBnmCD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53f32d37"
      },
      "source": [
        "Exercise 7.1\n",
        "\n",
        "Create a long transformation pipeline without any action.\n",
        "Tasks:\n",
        "Explain what Spark has done so far"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0efe4c8f"
      },
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "long_pipeline_df = (\n",
        "    rides_df\n",
        "    .filter(col(\"status\") == \"Completed\")\n",
        "    .filter(col(\"distance_km\") > 5)\n",
        "    .withColumn(\"duration_hours\", col(\"duration_seconds\") / 3600)\n",
        "    .select(\"ride_id\", \"city\", \"distance_km\", \"duration_hours\")\n",
        "    .filter(col(\"duration_hours\") < 1)\n",
        ")\n",
        "\n",
        "# No action is called here\n",
        "# long_pipeline_df.show()\n",
        "# long_pipeline_df.count()\n",
        "# long_pipeline_df.write.parquet(\"output\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 7.2\n",
        "\n",
        "Trigger different actions ( count , show , write ) separately.\n",
        "Tasks:\n",
        "Observe whether Spark recomputes the DAG\n",
        "Explain behavior"
      ],
      "metadata": {
        "id": "0rmmm3khpA2z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "376a9621"
      },
      "source": [
        "Exercise 7.2\n",
        "\n",
        "Trigger different actions ( count , show , write ) separately.\n",
        "Tasks:\n",
        "Observe whether Spark recomputes the DAG\n",
        "Explain behavior"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd39fb33",
        "outputId": "55b6522c-7620-4258-f796-ddc49100eca8"
      },
      "source": [
        "# Triggering count() action\n",
        "print(\"Count of rows in long_pipeline_df:\", long_pipeline_df.count())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of rows in long_pipeline_df: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "354f1902",
        "outputId": "4c37c1b7-e7f9-4871-85dc-15fdd08ae0b3"
      },
      "source": [
        "# Triggering show() action\n",
        "print(\"First 20 rows of long_pipeline_df:\")\n",
        "long_pipeline_df.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 20 rows of long_pipeline_df:\n",
            "+-------+---------+-----------+--------------------+\n",
            "|ride_id|     city|distance_km|      duration_hours|\n",
            "+-------+---------+-----------+--------------------+\n",
            "|   R001|Hyderabad|       12.5| 0.06666666666666667|\n",
            "|   R002|    Delhi|        8.2|                0.05|\n",
            "|   R004|Bangalore|        5.5| 0.03333333333333333|\n",
            "|   R005|Hyderabad|       20.0|                 0.1|\n",
            "|   R006|    Delhi|       25.0| 0.11666666666666667|\n",
            "|   R007|   Mumbai|        7.5|0.041666666666666664|\n",
            "|   R008|Bangalore|       18.0| 0.09166666666666666|\n",
            "|   R010|Hyderabad|       10.0| 0.05555555555555555|\n",
            "+-------+---------+-----------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96ab18c1",
        "outputId": "2eaac485-f978-464b-97b3-efb4f6750e8b"
      },
      "source": [
        "# Triggering write() action\n",
        "print(\"Writing long_pipeline_df to parquet...\")\n",
        "long_pipeline_df.write.mode(\"overwrite\").parquet(\"output_pipeline\")\n",
        "print(\"Write complete.\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing long_pipeline_df to parquet...\n",
            "Write complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EXERCISE SET 8 — THINKING QUESTIONS (WRITTEN)"
      ],
      "metadata": {
        "id": "CVyRKEE5nqCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why does broadcast remove shuffle from the DAG?\n",
        "\n",
        "Answer: Shuffle is a mechanism in Spark to repartition data across the cluster, typically to bring data with the same key together for operations like joins or aggregations. It involves significant network I/O, which is often a bottleneck.\n",
        "When you use a broadcast join (specifically BroadcastHashJoin), Spark sends the entire smaller DataFrame (or RDD) to all executor nodes in the cluster. This means that every executor now has a local copy of the smaller dataset. When the join operation then occurs, each executor can join its partition of the larger DataFrame with the local copy of the smaller DataFrame. Because the smaller dataset is already present on every node where the larger dataset's partitions reside, there's no need to exchange any data over the network to match keys for the larger DataFrame. This eliminates the need for a shuffle operation on the larger side of the join, making it highly efficient for joins with small lookup tables.\n",
        "\n",
        "2. Why does repartition always introduce shuffle?\n",
        "\n",
        "Answer: Repartition is an operation used to change the number of partitions of a DataFrame, or to redistribute data based on certain columns. When you invoke repartition, especially if you specify a different number of partitions or repartition by specific columns, Spark needs to ensure that the data is distributed correctly according to the new partitioning scheme.\n",
        "To achieve this, data from existing partitions must be moved to potentially new partitions across different executor nodes. For example, if you repartition by a column, all rows with the same value for that column must end up in the same new partition. This process of collecting data from various source partitions and redistributing it to target partitions across the network is inherently a shuffle operation. A shuffle involves serializing data, transferring it over the network, and then deserializing it on the receiving nodes, which is an expensive process.\n",
        "\n",
        "3. Why is coalesce cheaper than repartition?\n",
        "\n",
        "Answer: Both coalesce and repartition modify the number of partitions, but they do so differently, impacting their cost.\n",
        "Repartition can both increase and decrease the number of partitions, and it always performs a full shuffle. This means data from all existing partitions might be redistributed across the cluster to form the new partitions, even if the number of partitions is decreasing.\n",
        "Coalesce, in contrast, can only decrease the number of partitions. The key difference in its implementation is that it tries to avoid a full shuffle. Instead of redistributing all data, coalesce combines existing partitions on the same worker nodes. For instance, if you have 10 partitions and coalesce to 5, Spark might combine two existing partitions into one new partition on the same machine without moving data across the network. This minimizes data movement and network I/O, making coalesce a significantly cheaper operation than repartition when the goal is solely to reduce the number of partitions.\n",
        "\n",
        "4. Why does Spark delay execution until an action?\n",
        "\n",
        "Answer: Spark employs a fundamental concept known as lazy evaluation. This means that when you define transformations (such as filter, select, withColumn, join), Spark does not immediately execute these operations or process any data. Instead, it internally builds an execution plan, represented as a Directed Acyclic Graph (DAG), which outlines the sequence of operations that need to be performed.\n",
        "The actual computation (data processing) only begins when an action (like count, show, collect, write, saveAsTable) is invoked. Delaying execution until an action offers several significant advantages:\n",
        "Optimization: By having the complete lineage of transformations available before execution, Spark's Catalyst optimizer can analyze the entire DAG, reorder operations (e.g., push down filters to read less data), combine redundant steps, and apply various sophisticated optimizations to create a much more efficient physical execution plan. This can drastically reduce the amount of data read, shuffled, and processed.\n",
        "Fault Tolerance: If a node fails during an operation, Spark can intelligently recompute only the lost partitions from the last checkpoint or the original data source using the information in the DAG, rather than having to restart the entire computation from scratch.\n",
        "Resource Management: Knowing the full scope of work through the DAG allows Spark to better manage and allocate cluster resources (CPU, memory, network) more effectively for the entire job."
      ],
      "metadata": {
        "id": "PTfU15XLntQK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Rq6hwspnGF6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}