{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lit, when,regexp_replace, split, trim, array_compact, transform, get_json_object, lower\n",
        "spark = SparkSession.builder.appName(\"ride-hailing platform\").getOrCreate()\n",
        "from pyspark.sql.types import (StructType, StructField, StringType,LongType,IntegerType,ArrayType,MapType)"
      ],
      "metadata": {
        "id": "n7GrBZiItR20"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ASErbb6ts1qw"
      },
      "outputs": [],
      "source": [
        "raw_drivers = [\n",
        "(\"D001\",\"Ramesh\",\"35\",\"Hyderabad\",\"Car,Bike\"),\n",
        "(\"D002\",\"Suresh\",\"Forty\",\"Bangalore\",\"Auto\"),\n",
        "(\"D003\",\"Anita\",None,\"Mumbai\",[\"Car\"]),\n",
        "(\"D004\",\"Kiran\",\"29\",\"Delhi\",\"Car|Bike\"),\n",
        "(\"D005\",\"\", \"42\",\"Chennai\",None)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "driver_schema = StructType([\n",
        "    StructField(\"driverid\", StringType(), nullable=False),\n",
        "    StructField(\"name\", StringType(), nullable=True),\n",
        "    StructField(\"age\", StringType(), nullable=True),\n",
        "    StructField(\"city\", StringType(), nullable=True),\n",
        "    StructField(\"vechile\", StringType(), nullable=True)\n",
        "])\n",
        "df = spark.createDataFrame(raw_drivers,driver_schema)\n",
        "df.show()"
      ],
      "metadata": {
        "id": "ymtMrX6hxErG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "849b9fe3-0f27-4ad8-d2ab-cba569f5f8a8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+-----+---------+--------+\n",
            "|driverid|  name|  age|     city| vechile|\n",
            "+--------+------+-----+---------+--------+\n",
            "|    D001|Ramesh|   35|Hyderabad|Car,Bike|\n",
            "|    D002|Suresh|Forty|Bangalore|    Auto|\n",
            "|    D003| Anita| NULL|   Mumbai|   [Car]|\n",
            "|    D004| Kiran|   29|    Delhi|Car|Bike|\n",
            "|    D005|      |   42|  Chennai|    NULL|\n",
            "+--------+------+-----+---------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_age = df.withColumn(\"age\", when(col(\"age\") == \"\", None)\n",
        "    .when(col(\"age\").rlike(r\"^\\d+$\"),\n",
        "          col(\"age\").cast(IntegerType()))\n",
        "    .otherwise(None))\n",
        "\n",
        "clean_name_city_vechile = clean_age.withColumn(\"name\", when(col(\"name\") == \"\", None)\n",
        "    .otherwise(col(\"name\"))) \\\n",
        ".withColumn(\"city\",trim(col(\"city\")))\\\n",
        ".withColumn(\n",
        "    \"vechile\",\n",
        "    (when(\n",
        "        col(\"vechile\").isNull(),\n",
        "        None\n",
        "    ).otherwise(\n",
        "        array_compact(\n",
        "            transform(\n",
        "                split(\n",
        "                    regexp_replace(\n",
        "                        col(\"vechile\"),\n",
        "                        r\"\\[|\\]|'|\\|\", \",\"),\n",
        "                    \",\"),\n",
        "                lambda x: when(trim(x) != lit(\"\"), trim(x)).otherwise(lit(None))\n",
        "            )\n",
        "        )\n",
        "    )).cast(ArrayType(StringType()))\n",
        ")\n",
        "\n",
        "clean_name_city_vechile.show()\n",
        "\n",
        "driver_df = clean_name_city_vechile"
      ],
      "metadata": {
        "id": "fSH9j2Msw8is",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dacb38d-72ee-4025-ab36-3d153b68ff03"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+----+---------+-----------+\n",
            "|driverid|  name| age|     city|    vechile|\n",
            "+--------+------+----+---------+-----------+\n",
            "|    D001|Ramesh|  35|Hyderabad|[Car, Bike]|\n",
            "|    D002|Suresh|NULL|Bangalore|     [Auto]|\n",
            "|    D003| Anita|NULL|   Mumbai|      [Car]|\n",
            "|    D004| Kiran|  29|    Delhi|[Car, Bike]|\n",
            "|    D005|  NULL|  42|  Chennai|       NULL|\n",
            "+--------+------+----+---------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_cities = [\n",
        "(\"Hyderabad\",\"South\"),\n",
        "(\"Bangalore\",\"South\"),\n",
        "(\"Mumbai\",\"West\"),\n",
        "(\"Delhi\",\"North\"),\n",
        "(\"Chennai\",\"South\")\n",
        "]"
      ],
      "metadata": {
        "id": "vOmeLYy-yWa0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "city_schema = StructType([\n",
        "    StructField(\"city\", StringType(), nullable=True),\n",
        "    StructField(\"region\", StringType(), nullable=True)\n",
        "])\n",
        "city_df = spark.createDataFrame(raw_cities,city_schema)\n",
        "city_df.show()"
      ],
      "metadata": {
        "id": "pOvesvwSzeSF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0195b7b9-1aae-4c31-f134-0f8cb4955058"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------+\n",
            "|     city|region|\n",
            "+---------+------+\n",
            "|Hyderabad| South|\n",
            "|Bangalore| South|\n",
            "|   Mumbai|  West|\n",
            "|    Delhi| North|\n",
            "|  Chennai| South|\n",
            "+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Small reference dataset city_df\n",
        "\n",
        "Intended for broadcast join"
      ],
      "metadata": {
        "id": "6gZQX4XI0Dnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import broadcast"
      ],
      "metadata": {
        "id": "QHgxJCb_0XqD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "driver_join  = driver_df.join(broadcast(city_df), \"city\", \"inner\")\n",
        "driver_join.show()"
      ],
      "metadata": {
        "id": "4sonBQ6bz4x0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30103db4-f72e-4712-a168-598c4a183896"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+------+----+-----------+------+\n",
            "|     city|driverid|  name| age|    vechile|region|\n",
            "+---------+--------+------+----+-----------+------+\n",
            "|Hyderabad|    D001|Ramesh|  35|[Car, Bike]| South|\n",
            "|Bangalore|    D002|Suresh|NULL|     [Auto]| South|\n",
            "|   Mumbai|    D003| Anita|NULL|      [Car]|  West|\n",
            "|    Delhi|    D004| Kiran|  29|[Car, Bike]| North|\n",
            "|  Chennai|    D005|  NULL|  42|       NULL| South|\n",
            "+---------+--------+------+----+-----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_trips = [\n",
        "(\"T001\",\"D001\",\"Hyderabad\",\"2024-01-05\",\"Completed\",\"450\"),\n",
        "(\"T002\",\"D002\",\"Bangalore\",\"05/01/2024\",\"Cancelled\",\"0\"),\n",
        "(\"T003\",\"D003\",\"Mumbai\",\"2024/01/06\",\"Completed\",\"620\"),\n",
        "(\"T004\",\"D004\",\"Delhi\",\"invalid_date\",\"Completed\",\"540\"),\n",
        "(\"T005\",\"D001\",\"Hyderabad\",\"2024-01-10\",\"Completed\",\"700\"),\n",
        "(\"T006\",\"D005\",\"Chennai\",\"2024-01-12\",\"Completed\",\"350\")\n",
        "]"
      ],
      "metadata": {
        "id": "pHV7sDvK0lsq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART A — DATA CLEANING & STRUCTURING"
      ],
      "metadata": {
        "id": "Hew6ZM-wT_lN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trips_schema = StructType([\n",
        "    StructField(\"userid\", StringType(), nullable=False),\n",
        "    StructField(\"driverid\", StringType(), nullable=False),\n",
        "    StructField(\"city\", StringType(), nullable=True),\n",
        "    StructField(\"date\", StringType(), nullable=True),\n",
        "    StructField(\"status\", StringType(), nullable=True),\n",
        "    StructField(\"amount\", StringType(), nullable=True),\n",
        "])\n",
        "trips_df = spark.createDataFrame(raw_trips,trips_schema)\n",
        "trips_df.show()"
      ],
      "metadata": {
        "id": "4LsiSpMP0uy8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12ad2983-f742-4f4b-cc63-e03feb6b036a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------+------------+---------+------+\n",
            "|userid|driverid|     city|        date|   status|amount|\n",
            "+------+--------+---------+------------+---------+------+\n",
            "|  T001|    D001|Hyderabad|  2024-01-05|Completed|   450|\n",
            "|  T002|    D002|Bangalore|  05/01/2024|Cancelled|     0|\n",
            "|  T003|    D003|   Mumbai|  2024/01/06|Completed|   620|\n",
            "|  T004|    D004|    Delhi|invalid_date|Completed|   540|\n",
            "|  T005|    D001|Hyderabad|  2024-01-10|Completed|   700|\n",
            "|  T006|    D005|  Chennai|  2024-01-12|Completed|   350|\n",
            "+------+--------+---------+------------+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, to_date, coalesce, split, lit, array_remove, try_to_timestamp"
      ],
      "metadata": {
        "id": "lqBlF9Cf1Zmg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_date_amount = trips_df.withColumn(\"amount\", col(\"amount\").cast(IntegerType()))\\\n",
        ".withColumn(\n",
        "    \"date\",\n",
        "    coalesce(\n",
        "        to_date(try_to_timestamp(col(\"date\"), lit(\"yyyy-MM-dd\"))),\n",
        "        to_date(try_to_timestamp(col(\"date\"), lit(\"dd/MM/yyyy\"))),\n",
        "        to_date(try_to_timestamp(col(\"date\"), lit(\"yyyy/MM/dd\")))\n",
        "    )\n",
        ")\n",
        "\n",
        "clean_date_amount = clean_date_amount.filter(col(\"amount\") > 0)\n",
        "\n",
        "clean_date_amount.show()\n",
        "tripsdf=clean_date_amount"
      ],
      "metadata": {
        "id": "fLOOrDH51DIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e836ab9c-76e3-49a4-8b2b-eff66fd6f1c9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------+----------+---------+------+\n",
            "|userid|driverid|     city|      date|   status|amount|\n",
            "+------+--------+---------+----------+---------+------+\n",
            "|  T001|    D001|Hyderabad|2024-01-05|Completed|   450|\n",
            "|  T003|    D003|   Mumbai|2024-01-06|Completed|   620|\n",
            "|  T004|    D004|    Delhi|      NULL|Completed|   540|\n",
            "|  T005|    D001|Hyderabad|2024-01-10|Completed|   700|\n",
            "|  T006|    D005|  Chennai|2024-01-12|Completed|   350|\n",
            "+------+--------+---------+----------+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_activity = [\n",
        "(\"D001\",\"login,accept_trip,logout\",\"{'device':'mobile'}\",180),\n",
        "(\"D002\",[\"login\",\"logout\"],\"device=laptop\",60),\n",
        "(\"D003\",\"login|accept_trip\",None,120),\n",
        "(\"D004\",None,\"{'device':'tablet'}\",90),\n",
        "(\"D005\",\"login\",\"{'device':'mobile'}\",30)\n",
        "]"
      ],
      "metadata": {
        "id": "zuF572HI2xHt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activity_schema = StructType([\n",
        "    StructField(\"userid\", StringType(), nullable=False),\n",
        "    StructField(\"actions\", StringType(), nullable=True),\n",
        "    StructField(\"device\", StringType(), nullable=True),\n",
        "    StructField(\"amount\", IntegerType(), nullable=True),\n",
        "])\n",
        "activity_df = spark.createDataFrame(raw_activity,activity_schema)\n",
        "activity_df.show()"
      ],
      "metadata": {
        "id": "3rQUyG9Q1bTp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1d573ed-ecd6-447d-ff74-5eda379a8324"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+-------------------+------+\n",
            "|userid|             actions|             device|amount|\n",
            "+------+--------------------+-------------------+------+\n",
            "|  D001|login,accept_trip...|{'device':'mobile'}|   180|\n",
            "|  D002|     [login, logout]|      device=laptop|    60|\n",
            "|  D003|   login|accept_trip|               NULL|   120|\n",
            "|  D004|                NULL|{'device':'tablet'}|    90|\n",
            "|  D005|               login|{'device':'mobile'}|    30|\n",
            "+------+--------------------+-------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_activity_clean = activity_df.withColumn(\n",
        "    \"actions\",\n",
        "    (when(\n",
        "        col(\"actions\").isNull(),\n",
        "        None\n",
        "    ).otherwise(\n",
        "        array_compact(\n",
        "            transform(\n",
        "                split(\n",
        "                    regexp_replace(\n",
        "                        col(\"actions\"),\n",
        "                        r\"\\[|\\]|'|\\|\", \",\"),\n",
        "                    \",\"),\n",
        "                lambda x: when(trim(x) != lit(\"\"), trim(x)).otherwise(lit(None))\n",
        "            )\n",
        "        )\n",
        "    )).cast(ArrayType(StringType()))\n",
        ").withColumn(\n",
        "    \"device\",\n",
        "    when(col(\"device\").isNull(), None)\n",
        "    .when(col(\"device\").like(\"{'device':%}\"), get_json_object(col(\"device\"), \"$.device\"))\n",
        "    .when(col(\"device\").like(\"device=%\"), split(col(\"device\"), \"=\").getItem(1))\n",
        "    .otherwise(None)\n",
        ")\n",
        "\n",
        "df_activity_clean.show(truncate=False)\n",
        "df_activity_clean.printSchema()"
      ],
      "metadata": {
        "id": "YIMlaV643E7y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8296d42-46c4-44a2-e14f-9b1d8ff11a7f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------------------------+------+------+\n",
            "|userid|actions                     |device|amount|\n",
            "+------+----------------------------+------+------+\n",
            "|D001  |[login, accept_trip, logout]|mobile|180   |\n",
            "|D002  |[login, logout]             |laptop|60    |\n",
            "|D003  |[login, accept_trip]        |NULL  |120   |\n",
            "|D004  |NULL                        |tablet|90    |\n",
            "|D005  |[login]                     |mobile|30    |\n",
            "+------+----------------------------+------+------+\n",
            "\n",
            "root\n",
            " |-- userid: string (nullable = false)\n",
            " |-- actions: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- device: string (nullable = true)\n",
            " |-- amount: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_activity_clean.show()\n",
        "tripsdf.show()\n",
        "driver_join.show()\n",
        "driver_df.show()\n",
        "city_df.show()"
      ],
      "metadata": {
        "id": "1KYxNuNv3oDK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f16bd36-f9d7-4c86-cf9d-45cc151a4623"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+------+------+\n",
            "|userid|             actions|device|amount|\n",
            "+------+--------------------+------+------+\n",
            "|  D001|[login, accept_tr...|mobile|   180|\n",
            "|  D002|     [login, logout]|laptop|    60|\n",
            "|  D003|[login, accept_trip]|  NULL|   120|\n",
            "|  D004|                NULL|tablet|    90|\n",
            "|  D005|             [login]|mobile|    30|\n",
            "+------+--------------------+------+------+\n",
            "\n",
            "+------+--------+---------+----------+---------+------+\n",
            "|userid|driverid|     city|      date|   status|amount|\n",
            "+------+--------+---------+----------+---------+------+\n",
            "|  T001|    D001|Hyderabad|2024-01-05|Completed|   450|\n",
            "|  T003|    D003|   Mumbai|2024-01-06|Completed|   620|\n",
            "|  T004|    D004|    Delhi|      NULL|Completed|   540|\n",
            "|  T005|    D001|Hyderabad|2024-01-10|Completed|   700|\n",
            "|  T006|    D005|  Chennai|2024-01-12|Completed|   350|\n",
            "+------+--------+---------+----------+---------+------+\n",
            "\n",
            "+---------+--------+------+----+-----------+------+\n",
            "|     city|driverid|  name| age|    vechile|region|\n",
            "+---------+--------+------+----+-----------+------+\n",
            "|Hyderabad|    D001|Ramesh|  35|[Car, Bike]| South|\n",
            "|Bangalore|    D002|Suresh|NULL|     [Auto]| South|\n",
            "|   Mumbai|    D003| Anita|NULL|      [Car]|  West|\n",
            "|    Delhi|    D004| Kiran|  29|[Car, Bike]| North|\n",
            "|  Chennai|    D005|  NULL|  42|       NULL| South|\n",
            "+---------+--------+------+----+-----------+------+\n",
            "\n",
            "+--------+------+----+---------+-----------+\n",
            "|driverid|  name| age|     city|    vechile|\n",
            "+--------+------+----+---------+-----------+\n",
            "|    D001|Ramesh|  35|Hyderabad|[Car, Bike]|\n",
            "|    D002|Suresh|NULL|Bangalore|     [Auto]|\n",
            "|    D003| Anita|NULL|   Mumbai|      [Car]|\n",
            "|    D004| Kiran|  29|    Delhi|[Car, Bike]|\n",
            "|    D005|  NULL|  42|  Chennai|       NULL|\n",
            "+--------+------+----+---------+-----------+\n",
            "\n",
            "+---------+------+\n",
            "|     city|region|\n",
            "+---------+------+\n",
            "|Hyderabad| South|\n",
            "|Bangalore| South|\n",
            "|   Mumbai|  West|\n",
            "|    Delhi| North|\n",
            "|  Chennai| South|\n",
            "+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART B — DATA INTEGRATION (JOINS)\n"
      ],
      "metadata": {
        "id": "OqcI2OuY40Fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "city_df.show()\n",
        "\n",
        "trips_city_join  = tripsdf.join(broadcast(city_df), \"city\", \"inner\")\n",
        "trips_city_join.show()\n",
        "\n",
        "trips_city_join.explain(True)\n",
        "\n",
        "ophan = trips_city_join.filter(~trips_city_join[\"date\"].isNull())\n",
        "ophan.show()"
      ],
      "metadata": {
        "id": "yZa4-xaG4iT1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82d05dae-b1d9-4d5b-e5ce-681190c7ded8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------+\n",
            "|     city|region|\n",
            "+---------+------+\n",
            "|Hyderabad| South|\n",
            "|Bangalore| South|\n",
            "|   Mumbai|  West|\n",
            "|    Delhi| North|\n",
            "|  Chennai| South|\n",
            "+---------+------+\n",
            "\n",
            "+---------+------+--------+----------+---------+------+------+\n",
            "|     city|userid|driverid|      date|   status|amount|region|\n",
            "+---------+------+--------+----------+---------+------+------+\n",
            "|Hyderabad|  T001|    D001|2024-01-05|Completed|   450| South|\n",
            "|   Mumbai|  T003|    D003|2024-01-06|Completed|   620|  West|\n",
            "|    Delhi|  T004|    D004|      NULL|Completed|   540| North|\n",
            "|Hyderabad|  T005|    D001|2024-01-10|Completed|   700| South|\n",
            "|  Chennai|  T006|    D005|2024-01-12|Completed|   350| South|\n",
            "+---------+------+--------+----------+---------+------+------+\n",
            "\n",
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(Inner, [city])\n",
            ":- Filter (amount#97 > 0)\n",
            ":  +- Project [userid#72, driverid#73, city#74, coalesce(to_date(try_to_timestamp(date#75, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(date#75, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(date#75, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true)) AS date#98, status#76, amount#97]\n",
            ":     +- Project [userid#72, driverid#73, city#74, date#75, status#76, cast(amount#77 as int) AS amount#97]\n",
            ":        +- LogicalRDD [userid#72, driverid#73, city#74, date#75, status#76, amount#77], false\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- LogicalRDD [city#44, region#45], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, userid: string, driverid: string, date: date, status: string, amount: int, region: string\n",
            "Project [city#74, userid#72, driverid#73, date#98, status#76, amount#97, region#45]\n",
            "+- Join Inner, (city#74 = city#44)\n",
            "   :- Filter (amount#97 > 0)\n",
            "   :  +- Project [userid#72, driverid#73, city#74, coalesce(to_date(try_to_timestamp(date#75, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(date#75, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true), to_date(try_to_timestamp(date#75, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false), None, Some(Etc/UTC), true)) AS date#98, status#76, amount#97]\n",
            "   :     +- Project [userid#72, driverid#73, city#74, date#75, status#76, cast(amount#77 as int) AS amount#97]\n",
            "   :        +- LogicalRDD [userid#72, driverid#73, city#74, date#75, status#76, amount#77], false\n",
            "   +- ResolvedHint (strategy=broadcast)\n",
            "      +- LogicalRDD [city#44, region#45], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#74, userid#72, driverid#73, date#98, status#76, amount#97, region#45]\n",
            "+- Join Inner, (city#74 = city#44), rightHint=(strategy=broadcast)\n",
            "   :- Project [userid#72, driverid#73, city#74, coalesce(cast(gettimestamp(date#75, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(date#75, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(date#75, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS date#98, status#76, cast(amount#77 as int) AS amount#97]\n",
            "   :  +- Filter ((isnotnull(amount#77) AND (cast(amount#77 as int) > 0)) AND isnotnull(city#74))\n",
            "   :     +- LogicalRDD [userid#72, driverid#73, city#74, date#75, status#76, amount#77], false\n",
            "   +- Filter isnotnull(city#44)\n",
            "      +- LogicalRDD [city#44, region#45], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#74, userid#72, driverid#73, date#98, status#76, amount#97, region#45]\n",
            "   +- BroadcastHashJoin [city#74], [city#44], Inner, BuildRight, false\n",
            "      :- Project [userid#72, driverid#73, city#74, coalesce(cast(gettimestamp(date#75, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(date#75, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(date#75, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS date#98, status#76, cast(amount#77 as int) AS amount#97]\n",
            "      :  +- Filter ((isnotnull(amount#77) AND (cast(amount#77 as int) > 0)) AND isnotnull(city#74))\n",
            "      :     +- Scan ExistingRDD[userid#72,driverid#73,city#74,date#75,status#76,amount#77]\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=406]\n",
            "         +- Filter isnotnull(city#44)\n",
            "            +- Scan ExistingRDD[city#44,region#45]\n",
            "\n",
            "+---------+------+--------+----------+---------+------+------+\n",
            "|     city|userid|driverid|      date|   status|amount|region|\n",
            "+---------+------+--------+----------+---------+------+------+\n",
            "|Hyderabad|  T001|    D001|2024-01-05|Completed|   450| South|\n",
            "|   Mumbai|  T003|    D003|2024-01-06|Completed|   620|  West|\n",
            "|Hyderabad|  T005|    D001|2024-01-10|Completed|   700| South|\n",
            "|  Chennai|  T006|    D005|2024-01-12|Completed|   350| South|\n",
            "+---------+------+--------+----------+---------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART C — ANALYTICS & AGGREGATIONS"
      ],
      "metadata": {
        "id": "UIi7Qsq46YST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count\n",
        "\n",
        "total_trips_per_city = (\n",
        "    trips_df\n",
        "    .groupBy(\"city\")\n",
        "    .agg(count(\"*\").alias(\"Total revenue per city\"))\n",
        ")\n",
        "\n",
        "total_trips_per_city.show()\n"
      ],
      "metadata": {
        "id": "qgpVzMd15I-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5080ec9b-ac66-49ac-e0fe-e081a5c1526d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------------------+\n",
            "|     city|Total revenue per city|\n",
            "+---------+----------------------+\n",
            "|Bangalore|                     1|\n",
            "|   Mumbai|                     1|\n",
            "|Hyderabad|                     2|\n",
            "|  Chennai|                     1|\n",
            "|    Delhi|                     1|\n",
            "+---------+----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum\n",
        "\n",
        "total_rev_per_city = (\n",
        "    trips_df\n",
        "    .groupBy(\"city\")\n",
        "    .agg(sum(\"amount\").alias(\"total_trips\"))\n",
        ")\n",
        "\n",
        "total_rev_per_city.show()\n"
      ],
      "metadata": {
        "id": "ycuPaTrj5U-E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb4eda81-0de9-4b8d-827a-bae5bc2e7af2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------+\n",
            "|     city|total_trips|\n",
            "+---------+-----------+\n",
            "|Bangalore|        0.0|\n",
            "|   Mumbai|      620.0|\n",
            "|Hyderabad|     1150.0|\n",
            "|  Chennai|      350.0|\n",
            "|    Delhi|      540.0|\n",
            "+---------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count\n",
        "\n",
        "total_trips_completed = (\n",
        "    trips_df\n",
        "    .groupBy(\"driverid\")\n",
        "    .agg(count(\"*\").alias(\"total_trips_completed\"))\n",
        ")\n",
        "\n",
        "total_trips_completed.show()\n"
      ],
      "metadata": {
        "id": "SVQtGe2WWjjn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8cca380-8281-4195-f803-7c5d9ace26da"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------------------+\n",
            "|driverid|total_trips_completed|\n",
            "+--------+---------------------+\n",
            "|    D002|                    1|\n",
            "|    D003|                    1|\n",
            "|    D001|                    2|\n",
            "|    D004|                    1|\n",
            "|    D005|                    1|\n",
            "+--------+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trips_df.filter(trips_df[\"status\"] == \"Not Completed\").show()"
      ],
      "metadata": {
        "id": "0XRnoCA8WxjL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f257e60-a05f-4ed3-b98f-9646bfc46a14"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+----+----+------+------+\n",
            "|userid|driverid|city|date|status|amount|\n",
            "+------+--------+----+----+------+------+\n",
            "+------+--------+----+----+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART D — WINDOW FUNCTIONS"
      ],
      "metadata": {
        "id": "EwpcNjjB74MG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trips_df.show()"
      ],
      "metadata": {
        "id": "VH_MX7IX8ShO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9af31d9-c353-40a8-9255-6690330f882c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------+------------+---------+------+\n",
            "|userid|driverid|     city|        date|   status|amount|\n",
            "+------+--------+---------+------------+---------+------+\n",
            "|  T001|    D001|Hyderabad|  2024-01-05|Completed|   450|\n",
            "|  T002|    D002|Bangalore|  05/01/2024|Cancelled|     0|\n",
            "|  T003|    D003|   Mumbai|  2024/01/06|Completed|   620|\n",
            "|  T004|    D004|    Delhi|invalid_date|Completed|   540|\n",
            "|  T005|    D001|Hyderabad|  2024-01-10|Completed|   700|\n",
            "|  T006|    D005|  Chennai|  2024-01-12|Completed|   350|\n",
            "+------+--------+---------+------------+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "driver_revenue = trips_df.groupBy(\"driverid\") \\\n",
        "    .agg(F.sum(\"amount\").alias(\"total_revenue\")) \\\n",
        "    .orderBy(F.desc(\"total_revenue\"))\n",
        "\n",
        "driver_revenue.show()"
      ],
      "metadata": {
        "id": "rlYX0tne75JH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e308311-b215-48c0-fbea-4eed3da402e2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------------+\n",
            "|driverid|total_revenue|\n",
            "+--------+-------------+\n",
            "|    D001|       1150.0|\n",
            "|    D003|        620.0|\n",
            "|    D004|        540.0|\n",
            "|    D005|        350.0|\n",
            "|    D002|          0.0|\n",
            "+--------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window"
      ],
      "metadata": {
        "id": "Oug-Lbhb8iqK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "city_driver_rank = trips_df.groupBy(\"city\", \"driverid\") \\\n",
        "    .agg(F.sum(\"amount\").alias(\"city_revenue\")) \\\n",
        "    .withColumn(\"rank\", F.rank().over(Window.partitionBy(\"city\").orderBy(F.desc(\"city_revenue\"))))\n",
        "city_driver_rank.show()"
      ],
      "metadata": {
        "id": "THk0umdU76Us",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f0002b1-5c37-4957-e507-5b2f347e11d2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+------------+----+\n",
            "|     city|driverid|city_revenue|rank|\n",
            "+---------+--------+------------+----+\n",
            "|Bangalore|    D002|         0.0|   1|\n",
            "|  Chennai|    D005|       350.0|   1|\n",
            "|    Delhi|    D004|       540.0|   1|\n",
            "|Hyderabad|    D001|      1150.0|   1|\n",
            "|   Mumbai|    D003|       620.0|   1|\n",
            "+---------+--------+------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "\n",
        "city_date_window = Window.partitionBy(\"city\").orderBy(\"date\") \\\n",
        "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "\n",
        "running_revenue = trips_df.groupBy(\"city\", \"date\") \\\n",
        "    .agg(F.sum(\"amount\").alias(\"daily_revenue\")) \\\n",
        "    .withColumn(\"running_revenue\", F.sum(\"daily_revenue\").over(city_date_window))"
      ],
      "metadata": {
        "id": "VlXMLWXk8kHo"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ". Compare GroupBy vs Window for one metric\n",
        "GroupBy: Aggregates data into fewer rows (e.g., total revenue per driver).\n",
        "Window: Keeps original granularity but adds computed columns (e.g., rank, cumulative sum).\n",
        "Use Case:\n",
        "GroupBy → summary reports.\n",
        "Window → analytics like ranking, running totals without collapsing rows"
      ],
      "metadata": {
        "id": "Q1BfWmA98-96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART E — UDF (ONLY IF REQUIRED)"
      ],
      "metadata": {
        "id": "YEMO9tMx9WkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import udf, col\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Define classification based on revenue\n",
        "def classify_revenue(revenue):\n",
        "    if revenue >= 1000:\n",
        "        return \"High\"\n",
        "    elif revenue >= 500:\n",
        "        return \"Medium\"\n",
        "    else:\n",
        "        return \"Low\"\n",
        "\n",
        "# Register UDF\n",
        "classify_revenue_udf = udf(classify_revenue, StringType())\n",
        "\n",
        "# Apply UDF on city_revenue column\n",
        "city_driver_rank.withColumn(\"revenue_grade\", classify_revenue_udf(col(\"city_revenue\"))).show()\n"
      ],
      "metadata": {
        "id": "DFLOfixe894m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b96c29c1-ece0-4e4d-c9c0-ac26ab7249b7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+------------+----+-------------+\n",
            "|     city|driverid|city_revenue|rank|revenue_grade|\n",
            "+---------+--------+------------+----+-------------+\n",
            "|Bangalore|    D002|         0.0|   1|          Low|\n",
            "|  Chennai|    D005|       350.0|   1|          Low|\n",
            "|    Delhi|    D004|       540.0|   1|       Medium|\n",
            "|Hyderabad|    D001|      1150.0|   1|         High|\n",
            "|   Mumbai|    D003|       620.0|   1|       Medium|\n",
            "+---------+--------+------------+----+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART F — SORTING & ORDERING"
      ],
      "metadata": {
        "id": "o93gVPSQVP9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import desc\n",
        "\n",
        "sorted_cities_by_revenue = (\n",
        "     trips_df\n",
        "    .groupBy(\"city\")\n",
        "    .agg(sum(\"amount\").alias(\"total_revenue\"))\n",
        "    .orderBy(desc(\"total_revenue\"))\n",
        ")\n",
        "\n",
        "sorted_cities_by_revenue.show()\n"
      ],
      "metadata": {
        "id": "d40PCDd9XWXV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc5ddc5d-98a9-4374-ddf3-27ff3be82e16"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------+\n",
            "|     city|total_revenue|\n",
            "+---------+-------------+\n",
            "|Hyderabad|       1150.0|\n",
            "|   Mumbai|        620.0|\n",
            "|    Delhi|        540.0|\n",
            "|  Chennai|        350.0|\n",
            "|Bangalore|          0.0|\n",
            "+---------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum\n",
        "\n",
        "driver_city_revenue = (\n",
        "    trips_df\n",
        "    .groupBy(\"city\", \"driverid\")\n",
        "    .agg(sum(\"amount\").alias(\"driver_revenue\"))\n",
        ")\n",
        "\n",
        "driver_city_revenue.show()\n",
        "\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import desc, row_number\n",
        "\n",
        "city_window = Window.partitionBy(\"city\").orderBy(desc(\"driver_revenue\"))\n",
        "\n",
        "sorted_drivers_within_city = (\n",
        "    driver_city_revenue\n",
        "    .withColumn(\"rank\", row_number().over(city_window))\n",
        "    .orderBy(\"city\", \"rank\")\n",
        ")\n",
        "\n",
        "sorted_drivers_within_city.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "ToKVPfp1XXhd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3ce2189-fce1-42f7-c60a-a3e73c34b4e5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+--------------+\n",
            "|     city|driverid|driver_revenue|\n",
            "+---------+--------+--------------+\n",
            "|Bangalore|    D002|           0.0|\n",
            "|   Mumbai|    D003|         620.0|\n",
            "|Hyderabad|    D001|        1150.0|\n",
            "|  Chennai|    D005|         350.0|\n",
            "|    Delhi|    D004|         540.0|\n",
            "+---------+--------+--------------+\n",
            "\n",
            "+---------+--------+--------------+----+\n",
            "|     city|driverid|driver_revenue|rank|\n",
            "+---------+--------+--------------+----+\n",
            "|Bangalore|    D002|           0.0|   1|\n",
            "|  Chennai|    D005|         350.0|   1|\n",
            "|    Delhi|    D004|         540.0|   1|\n",
            "|Hyderabad|    D001|        1150.0|   1|\n",
            "|   Mumbai|    D003|         620.0|   1|\n",
            "+---------+--------+--------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sorting causes a shuffle because Spark must move data across partitions to establish a global or partition-level order."
      ],
      "metadata": {
        "id": "unqsShy2Y8Uc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART G — SET OPERATIONS"
      ],
      "metadata": {
        "id": "ov-wqLmpVVeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "completed_drivers_df = (\n",
        "    trips_df\n",
        "    .filter(trips_df.status == \"Completed\")\n",
        "    .select(\"driverid\")\n",
        "    .distinct()\n",
        ")\n",
        "\n",
        "completed_drivers_df.show()\n"
      ],
      "metadata": {
        "id": "fiiIAe1tY_UH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7552fab0-9e53-4e94-fb29-bb4b9635d36e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|driverid|\n",
            "+--------+\n",
            "|    D003|\n",
            "|    D001|\n",
            "|    D004|\n",
            "|    D005|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_activity_clean.show(truncate=False)"
      ],
      "metadata": {
        "id": "Zzl8EfZwZU60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a53c82e-7acc-4dcf-b203-c2e54ef4fd14"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------------------------+------+------+\n",
            "|userid|actions                     |device|amount|\n",
            "+------+----------------------------+------+------+\n",
            "|D001  |[login, accept_trip, logout]|mobile|180   |\n",
            "|D002  |[login, logout]             |laptop|60    |\n",
            "|D003  |[login, accept_trip]        |NULL  |120   |\n",
            "|D004  |NULL                        |tablet|90    |\n",
            "|D005  |[login]                     |mobile|30    |\n",
            "+------+----------------------------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import array_contains\n",
        "\n",
        "active_drivers_df = (\n",
        "    df_activity_clean\n",
        "    .filter(array_contains(\"actions\", \"login\")  & ~array_contains(\"actions\", \"logout\"))\n",
        "    .select(\"userid\")\n",
        "    .distinct()\n",
        ")\n",
        "\n",
        "active_drivers_df.show()\n"
      ],
      "metadata": {
        "id": "OqAi5sfDY_AJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "080a4f51-d251-4e62-cb23-5da7507b11a6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|userid|\n",
            "+------+\n",
            "|  D003|\n",
            "|  D005|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set operations work on entire rows and treat DataFrames as mathematical sets, while joins combine columns based on matching keys."
      ],
      "metadata": {
        "id": "i9Fds02naBYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART H — DAG & PERFORMANCE ANALYSIS"
      ],
      "metadata": {
        "id": "LrSkq_wRVa5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trip_city_join_df = trips_df.join(\n",
        "    city_df,\n",
        "    on=\"city\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "trip_city_join_df.explain(True)"
      ],
      "metadata": {
        "id": "vJ3xJpkTZCJA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16d1e383-3dcd-469e-fe69-f088cf5422de"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(LeftOuter, [city])\n",
            ":- LogicalRDD [userid#72, driverid#73, city#74, date#75, status#76, amount#77], false\n",
            "+- LogicalRDD [city#44, region#45], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, userid: string, driverid: string, date: string, status: string, amount: string, region: string\n",
            "Project [city#74, userid#72, driverid#73, date#75, status#76, amount#77, region#45]\n",
            "+- Join LeftOuter, (city#74 = city#44)\n",
            "   :- LogicalRDD [userid#72, driverid#73, city#74, date#75, status#76, amount#77], false\n",
            "   +- LogicalRDD [city#44, region#45], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#74, userid#72, driverid#73, date#75, status#76, amount#77, region#45]\n",
            "+- Join LeftOuter, (city#74 = city#44)\n",
            "   :- LogicalRDD [userid#72, driverid#73, city#74, date#75, status#76, amount#77], false\n",
            "   +- Filter isnotnull(city#44)\n",
            "      +- LogicalRDD [city#44, region#45], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#74, userid#72, driverid#73, date#75, status#76, amount#77, region#45]\n",
            "   +- SortMergeJoin [city#74], [city#44], LeftOuter\n",
            "      :- Sort [city#74 ASC NULLS FIRST], false, 0\n",
            "      :  +- Exchange hashpartitioning(city#74, 200), ENSURE_REQUIREMENTS, [plan_id=1330]\n",
            "      :     +- Scan ExistingRDD[userid#72,driverid#73,city#74,date#75,status#76,amount#77]\n",
            "      +- Sort [city#44 ASC NULLS FIRST], false, 0\n",
            "         +- Exchange hashpartitioning(city#44, 200), ENSURE_REQUIREMENTS, [plan_id=1331]\n",
            "            +- Filter isnotnull(city#44)\n",
            "               +- Scan ExistingRDD[city#44,region#45]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_drivers_within_city.explain(True)\n"
      ],
      "metadata": {
        "id": "2F1ZLHWdZB7H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c3497e2-5f6f-45e6-ca7a-c73a8853b6b6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Sort ['city ASC NULLS FIRST, 'rank ASC NULLS FIRST], true\n",
            "+- Project [city#74, driverid#73, driver_revenue#471, rank#492]\n",
            "   +- Project [city#74, driverid#73, driver_revenue#471, rank#492, rank#492]\n",
            "      +- Window [row_number() windowspecdefinition(city#74, driver_revenue#471 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#492], [city#74], [driver_revenue#471 DESC NULLS LAST]\n",
            "         +- Project [city#74, driverid#73, driver_revenue#471]\n",
            "            +- Aggregate [city#74, driverid#73], [city#74, driverid#73, sum(cast(amount#77 as double)) AS driver_revenue#471]\n",
            "               +- LogicalRDD [userid#72, driverid#73, city#74, date#75, status#76, amount#77], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, driverid: string, driver_revenue: double, rank: int\n",
            "Sort [city#74 ASC NULLS FIRST, rank#492 ASC NULLS FIRST], true\n",
            "+- Project [city#74, driverid#73, driver_revenue#471, rank#492]\n",
            "   +- Project [city#74, driverid#73, driver_revenue#471, rank#492, rank#492]\n",
            "      +- Window [row_number() windowspecdefinition(city#74, driver_revenue#471 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#492], [city#74], [driver_revenue#471 DESC NULLS LAST]\n",
            "         +- Project [city#74, driverid#73, driver_revenue#471]\n",
            "            +- Aggregate [city#74, driverid#73], [city#74, driverid#73, sum(cast(amount#77 as double)) AS driver_revenue#471]\n",
            "               +- LogicalRDD [userid#72, driverid#73, city#74, date#75, status#76, amount#77], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Sort [city#74 ASC NULLS FIRST, rank#492 ASC NULLS FIRST], true\n",
            "+- Window [row_number() windowspecdefinition(city#74, driver_revenue#471 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#492], [city#74], [driver_revenue#471 DESC NULLS LAST]\n",
            "   +- Aggregate [city#74, driverid#73], [city#74, driverid#73, sum(cast(amount#77 as double)) AS driver_revenue#471]\n",
            "      +- Project [driverid#73, city#74, amount#77]\n",
            "         +- LogicalRDD [userid#72, driverid#73, city#74, date#75, status#76, amount#77], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Sort [city#74 ASC NULLS FIRST, rank#492 ASC NULLS FIRST], true, 0\n",
            "   +- Exchange rangepartitioning(city#74 ASC NULLS FIRST, rank#492 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=1366]\n",
            "      +- Window [row_number() windowspecdefinition(city#74, driver_revenue#471 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#492], [city#74], [driver_revenue#471 DESC NULLS LAST]\n",
            "         +- Sort [city#74 ASC NULLS FIRST, driver_revenue#471 DESC NULLS LAST], false, 0\n",
            "            +- Exchange hashpartitioning(city#74, 200), ENSURE_REQUIREMENTS, [plan_id=1362]\n",
            "               +- HashAggregate(keys=[city#74, driverid#73], functions=[sum(cast(amount#77 as double))], output=[city#74, driverid#73, driver_revenue#471])\n",
            "                  +- Exchange hashpartitioning(city#74, driverid#73, 200), ENSURE_REQUIREMENTS, [plan_id=1359]\n",
            "                     +- HashAggregate(keys=[city#74, driverid#73], functions=[partial_sum(cast(amount#77 as double))], output=[city#74, driverid#73, sum#483])\n",
            "                        +- Project [driverid#73, city#74, amount#77]\n",
            "                           +- Scan ExistingRDD[userid#72,driverid#73,city#74,date#75,status#76,amount#77]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "optimized_join_df = trips_df.join(\n",
        "    broadcast(city_df),\n",
        "    on=\"city\",\n",
        "    how=\"left\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "IiDtDGrI8_S7"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Identifying Shuffles, Broadcast Joins, and Sort Stages ---\\n\")\n",
        "\n",
        "# 1. Shuffles (often occur with wide transformations like non-broadcast joins, groupBy, orderBy)\n",
        "# For example, a non-broadcast join like trip_city_join_df will typically involve shuffles (SortMergeJoin)\n",
        "print(\"\\n--- Shuffles (e.g., in trip_city_join_df) ---\")\n",
        "trip_city_join_df.explain(True)\n",
        "print(\"\\nNote: In the Physical Plan above, look for 'Exchange hashpartitioning' or 'Exchange rangepartitioning' which indicate a shuffle.\")\n",
        "print(\"A 'SortMergeJoin' also implies shuffles for sorting before merging.\")\n",
        "\n",
        "# Another example for shuffles and sorts from window functions/orderBy\n",
        "print(\"\\n--- Shuffles and Sorts (e.g., in sorted_drivers_within_city) ---\")\n",
        "sorted_drivers_within_city.explain(True)\n",
        "print(\"\\nNote: In the Physical Plan above, 'Exchange rangepartitioning' and 'Sort' operations are prominent, indicating shuffles and sorts.\")\n",
        "\n",
        "# 2. Broadcast Joins (explicitly used with `broadcast` hint)\n",
        "print(\"\\n--- Broadcast Joins (e.g., in optimized_join_df) ---\")\n",
        "optimized_join_df.explain(True)\n",
        "print(\"\\nNote: In the Physical Plan above, look for 'BroadcastHashJoin' and 'BroadcastExchange' which confirm a broadcast join.\")\n",
        "\n",
        "# 3. Sort Stages (occur with orderBy, window functions requiring order, or SortMergeJoin)\n",
        "print(\"\\n--- Sort Stages (e.g., in trip_city_join_df for SortMergeJoin) ---\")\n",
        "trip_city_join_df.explain(True)\n",
        "print(\"\\nNote: In the Physical Plan above, 'Sort' within 'SortMergeJoin' indicates sort stages.\")\n",
        "\n",
        "print(\"\\n--- Sort Stages (e.g., in sorted_drivers_within_city for window functions and final ordering) ---\")\n",
        "sorted_drivers_within_city.explain(True)\n",
        "print(\"\\nNote: In the Physical Plan above, 'Sort' is explicitly shown for ordering the data within partitions before the window function, and again for the final global sort.\")"
      ],
      "metadata": {
        "id": "PFLiShSqamDs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ed51946-c98b-4444-8ab6-8ec1ce7e016f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Identifying Shuffles, Broadcast Joins, and Sort Stages ---\n",
            "\n",
            "\n",
            "--- Shuffles (e.g., in trip_city_join_df) ---\n",
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(LeftOuter, [city])\n",
            ":- LogicalRDD [userid#72, driverid#73, city#74, date#75, status#76, amount#77], false\n",
            "+- LogicalRDD [city#44, region#45], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, userid: string, driverid: string, date: string, status: string, amount: string, region: string\n",
            "Project [city#74, userid#72, driverid#73, date#75, status#76, amount#77, region#45]\n",
            "+- Join LeftOuter, (city#74 = city#44)\n",
            "   :- LogicalRDD [userid#72, driverid#73, city#74, date#75, status#76, amount#77], false\n",
            "   +- LogicalRDD [city#44, region#45], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#74, userid#72, driverid#73, date#75, status#76, amount#77, region#45]\n",
            "+- Join LeftOuter, (city#74 = city#44)\n",
            "   :- LogicalRDD [userid#72, driverid#73, city#74, date#75, status#76, amount#77], false\n",
            "   +- Filter isnotnull(city#44)\n",
            "      +- LogicalRDD [city#44, region#45], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#74, userid#72, driverid#73, date#75, status#76, amount#77, region#45]\n",
            "   +- SortMergeJoin [city#74], [city#44], LeftOuter\n",
            "      :- Sort [city#74 ASC NULLS FIRST], false, 0\n",
            "      :  +- Exchange hashpartitioning(city#74, 200), ENSURE_REQUIREMENTS, [plan_id=1330]\n",
            "      :     +- Scan ExistingRDD[userid#72,driverid#73,city#74,date#75,status#76,amount#77]\n",
            "      +- Sort [city#44 ASC NULLS FIRST], false, 0\n",
            "         +- Exchange hashpartitioning(city#44, 200), ENSURE_REQUIREMENTS, [plan_id=1331]\n",
            "            +- Filter isnotnull(city#44)\n",
            "               +- Scan ExistingRDD[city#44,region#45]\n",
            "\n",
            "\n",
            "Note: In the Physical Plan above, look for 'Exchange hashpartitioning' or 'Exchange rangepartitioning' which indicate a shuffle.\n",
            "A 'SortMergeJoin' also implies shuffles for sorting before merging.\n",
            "\n",
            "--- Shuffles and Sorts (e.g., in sorted_drivers_within_city) ---\n",
            "== Parsed Logical Plan ==\n",
            "'Sort ['city ASC NULLS FIRST, 'rank ASC NULLS FIRST], true\n",
            "+- Project [city#74, driverid#73, driver_revenue#471, rank#492]\n",
            "   +- Project [city#74, driverid#73, driver_revenue#471, rank#492, rank#492]\n",
            "      +- Window [row_number() windowspecdefinition(city#74, driver_revenue#471 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#492], [city#74], [driver_revenue#471 DESC NULLS LAST]\n",
            "         +- Project [city#74, driverid#73, driver_revenue#471]\n",
            "            +- Aggregate [city#74, driverid#73], [city#74, driverid#73, sum(cast(amount#77 as double)) AS driver_revenue#471]\n",
            "               +- LogicalRDD [userid#72, driverid#73, city#74, date#75, status#76, amount#77], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, driverid: string, driver_revenue: double, rank: int\n",
            "Sort [city#74 ASC NULLS FIRST, rank#492 ASC NULLS FIRST], true\n",
            "+- Project [city#74, driverid#73, driver_revenue#471, rank#492]\n",
            "   +- Project [city#74, driverid#73, driver_revenue#471, rank#492, rank#492]\n",
            "      +- Window [row_number() windowspecdefinition(city#74, driver_revenue#471 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#492], [city#74], [driver_revenue#471 DESC NULLS LAST]\n",
            "         +- Project [city#74, driverid#73, driver_revenue#471]\n",
            "            +- Aggregate [city#74, driverid#73], [city#74, driverid#73, sum(cast(amount#77 as double)) AS driver_revenue#471]\n",
            "               +- LogicalRDD [userid#72, driverid#73, city#74, date#75, status#76, amount#77], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Sort [city#74 ASC NULLS FIRST, rank#492 ASC NULLS FIRST], true\n",
            "+- Window [row_number() windowspecdefinition(city#74, driver_revenue#471 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#492], [city#74], [driver_revenue#471 DESC NULLS LAST]\n",
            "   +- Aggregate [city#74, driverid#73], [city#74, driverid#73, sum(cast(amount#77 as double)) AS driver_revenue#471]\n",
            "      +- Project [driverid#73, city#74, amount#77]\n",
            "         +- LogicalRDD [userid#72, driverid#73, city#74, date#75, status#76, amount#77], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Sort [city#74 ASC NULLS FIRST, rank#492 ASC NULLS FIRST], true, 0\n",
            "   +- Exchange rangepartitioning(city#74 ASC NULLS FIRST, rank#492 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=1366]\n",
            "      +- Window [row_number() windowspecdefinition(city#74, driver_revenue#471 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#492], [city#74], [driver_revenue#471 DESC NULLS LAST]\n",
            "         +- Sort [city#74 ASC NULLS FIRST, driver_revenue#471 DESC NULLS LAST], false, 0\n",
            "            +- Exchange hashpartitioning(city#74, 200), ENSURE_REQUIREMENTS, [plan_id=1362]\n",
            "               +- HashAggregate(keys=[city#74, driverid#73], functions=[sum(cast(amount#77 as double))], output=[city#74, driverid#73, driver_revenue#471])\n",
            "                  +- Exchange hashpartitioning(city#74, driverid#73, 200), ENSURE_REQUIREMENTS, [plan_id=1359]\n",
            "                     +- HashAggregate(keys=[city#74, driverid#73], functions=[partial_sum(cast(amount#77 as double))], output=[city#74, driverid#73, sum#483])\n",
            "                        +- Project [driverid#73, city#74, amount#77]\n",
            "                           +- Scan ExistingRDD[userid#72,driverid#73,city#74,date#75,status#76,amount#77]\n",
            "\n",
            "\n",
            "Note: In the Physical Plan above, 'Exchange rangepartitioning' and 'Sort' operations are prominent, indicating shuffles and sorts.\n",
            "\n",
            "--- Broadcast Joins (e.g., in optimized_join_df) ---\n",
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(LeftOuter, [city])\n",
            ":- LogicalRDD [userid#72, driverid#73, city#74, date#75, status#76, amount#77], false\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- LogicalRDD [city#44, region#45], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, userid: string, driverid: string, date: string, status: string, amount: string, region: string\n",
            "Project [city#74, userid#72, driverid#73, date#75, status#76, amount#77, region#45]\n",
            "+- Join LeftOuter, (city#74 = city#44)\n",
            "   :- LogicalRDD [userid#72, driverid#73, city#74, date#75, status#76, amount#77], false\n",
            "   +- ResolvedHint (strategy=broadcast)\n",
            "      +- LogicalRDD [city#44, region#45], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#74, userid#72, driverid#73, date#75, status#76, amount#77, region#45]\n",
            "+- Join LeftOuter, (city#74 = city#44), rightHint=(strategy=broadcast)\n",
            "   :- LogicalRDD [userid#72, driverid#73, city#74, date#75, status#76, amount#77], false\n",
            "   +- Filter isnotnull(city#44)\n",
            "      +- LogicalRDD [city#44, region#45], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#74, userid#72, driverid#73, date#75, status#76, amount#77, region#45]\n",
            "   +- BroadcastHashJoin [city#74], [city#44], LeftOuter, BuildRight, false\n",
            "      :- Scan ExistingRDD[userid#72,driverid#73,city#74,date#75,status#76,amount#77]\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=1388]\n",
            "         +- Filter isnotnull(city#44)\n",
            "            +- Scan ExistingRDD[city#44,region#45]\n",
            "\n",
            "\n",
            "Note: In the Physical Plan above, look for 'BroadcastHashJoin' and 'BroadcastExchange' which confirm a broadcast join.\n",
            "\n",
            "--- Sort Stages (e.g., in trip_city_join_df for SortMergeJoin) ---\n",
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(LeftOuter, [city])\n",
            ":- LogicalRDD [userid#72, driverid#73, city#74, date#75, status#76, amount#77], false\n",
            "+- LogicalRDD [city#44, region#45], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, userid: string, driverid: string, date: string, status: string, amount: string, region: string\n",
            "Project [city#74, userid#72, driverid#73, date#75, status#76, amount#77, region#45]\n",
            "+- Join LeftOuter, (city#74 = city#44)\n",
            "   :- LogicalRDD [userid#72, driverid#73, city#74, date#75, status#76, amount#77], false\n",
            "   +- LogicalRDD [city#44, region#45], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#74, userid#72, driverid#73, date#75, status#76, amount#77, region#45]\n",
            "+- Join LeftOuter, (city#74 = city#44)\n",
            "   :- LogicalRDD [userid#72, driverid#73, city#74, date#75, status#76, amount#77], false\n",
            "   +- Filter isnotnull(city#44)\n",
            "      +- LogicalRDD [city#44, region#45], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#74, userid#72, driverid#73, date#75, status#76, amount#77, region#45]\n",
            "   +- SortMergeJoin [city#74], [city#44], LeftOuter\n",
            "      :- Sort [city#74 ASC NULLS FIRST], false, 0\n",
            "      :  +- Exchange hashpartitioning(city#74, 200), ENSURE_REQUIREMENTS, [plan_id=1330]\n",
            "      :     +- Scan ExistingRDD[userid#72,driverid#73,city#74,date#75,status#76,amount#77]\n",
            "      +- Sort [city#44 ASC NULLS FIRST], false, 0\n",
            "         +- Exchange hashpartitioning(city#44, 200), ENSURE_REQUIREMENTS, [plan_id=1331]\n",
            "            +- Filter isnotnull(city#44)\n",
            "               +- Scan ExistingRDD[city#44,region#45]\n",
            "\n",
            "\n",
            "Note: In the Physical Plan above, 'Sort' within 'SortMergeJoin' indicates sort stages.\n",
            "\n",
            "--- Sort Stages (e.g., in sorted_drivers_within_city for window functions and final ordering) ---\n",
            "== Parsed Logical Plan ==\n",
            "'Sort ['city ASC NULLS FIRST, 'rank ASC NULLS FIRST], true\n",
            "+- Project [city#74, driverid#73, driver_revenue#471, rank#492]\n",
            "   +- Project [city#74, driverid#73, driver_revenue#471, rank#492, rank#492]\n",
            "      +- Window [row_number() windowspecdefinition(city#74, driver_revenue#471 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#492], [city#74], [driver_revenue#471 DESC NULLS LAST]\n",
            "         +- Project [city#74, driverid#73, driver_revenue#471]\n",
            "            +- Aggregate [city#74, driverid#73], [city#74, driverid#73, sum(cast(amount#77 as double)) AS driver_revenue#471]\n",
            "               +- LogicalRDD [userid#72, driverid#73, city#74, date#75, status#76, amount#77], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, driverid: string, driver_revenue: double, rank: int\n",
            "Sort [city#74 ASC NULLS FIRST, rank#492 ASC NULLS FIRST], true\n",
            "+- Project [city#74, driverid#73, driver_revenue#471, rank#492]\n",
            "   +- Project [city#74, driverid#73, driver_revenue#471, rank#492, rank#492]\n",
            "      +- Window [row_number() windowspecdefinition(city#74, driver_revenue#471 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#492], [city#74], [driver_revenue#471 DESC NULLS LAST]\n",
            "         +- Project [city#74, driverid#73, driver_revenue#471]\n",
            "            +- Aggregate [city#74, driverid#73], [city#74, driverid#73, sum(cast(amount#77 as double)) AS driver_revenue#471]\n",
            "               +- LogicalRDD [userid#72, driverid#73, city#74, date#75, status#76, amount#77], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Sort [city#74 ASC NULLS FIRST, rank#492 ASC NULLS FIRST], true\n",
            "+- Window [row_number() windowspecdefinition(city#74, driver_revenue#471 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#492], [city#74], [driver_revenue#471 DESC NULLS LAST]\n",
            "   +- Aggregate [city#74, driverid#73], [city#74, driverid#73, sum(cast(amount#77 as double)) AS driver_revenue#471]\n",
            "      +- Project [driverid#73, city#74, amount#77]\n",
            "         +- LogicalRDD [userid#72, driverid#73, city#74, date#75, status#76, amount#77], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Sort [city#74 ASC NULLS FIRST, rank#492 ASC NULLS FIRST], true, 0\n",
            "   +- Exchange rangepartitioning(city#74 ASC NULLS FIRST, rank#492 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=1366]\n",
            "      +- Window [row_number() windowspecdefinition(city#74, driver_revenue#471 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#492], [city#74], [driver_revenue#471 DESC NULLS LAST]\n",
            "         +- Sort [city#74 ASC NULLS FIRST, driver_revenue#471 DESC NULLS LAST], false, 0\n",
            "            +- Exchange hashpartitioning(city#74, 200), ENSURE_REQUIREMENTS, [plan_id=1362]\n",
            "               +- HashAggregate(keys=[city#74, driverid#73], functions=[sum(cast(amount#77 as double))], output=[city#74, driverid#73, driver_revenue#471])\n",
            "                  +- Exchange hashpartitioning(city#74, driverid#73, 200), ENSURE_REQUIREMENTS, [plan_id=1359]\n",
            "                     +- HashAggregate(keys=[city#74, driverid#73], functions=[partial_sum(cast(amount#77 as double))], output=[city#74, driverid#73, sum#483])\n",
            "                        +- Project [driverid#73, city#74, amount#77]\n",
            "                           +- Scan ExistingRDD[userid#72,driverid#73,city#74,date#75,status#76,amount#77]\n",
            "\n",
            "\n",
            "Note: In the Physical Plan above, 'Sort' is explicitly shown for ordering the data within partitions before the window function, and again for the final global sort.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "# Assuming trips_df is the larger DataFrame and city_df is the smaller one\n",
        "# This will broadcast city_df to all worker nodes to avoid shuffling trips_df\n",
        "optimized_trips_city_join = trips_df.join(broadcast(city_df), on=\"city\", how=\"inner\")\n",
        "\n",
        "print(\"\\n--- Optimized Join using Broadcast Hint ---\")\n",
        "optimized_trips_city_join.explain(True)\n",
        "optimized_trips_city_join.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNubDF3aVIUF",
        "outputId": "f4e3ca8e-f808-4258-a6d6-62853c753e08"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Optimized Join using Broadcast Hint ---\n",
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(Inner, [city])\n",
            ":- LogicalRDD [userid#72, driverid#73, city#74, date#75, status#76, amount#77], false\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- LogicalRDD [city#44, region#45], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, userid: string, driverid: string, date: string, status: string, amount: string, region: string\n",
            "Project [city#74, userid#72, driverid#73, date#75, status#76, amount#77, region#45]\n",
            "+- Join Inner, (city#74 = city#44)\n",
            "   :- LogicalRDD [userid#72, driverid#73, city#74, date#75, status#76, amount#77], false\n",
            "   +- ResolvedHint (strategy=broadcast)\n",
            "      +- LogicalRDD [city#44, region#45], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#74, userid#72, driverid#73, date#75, status#76, amount#77, region#45]\n",
            "+- Join Inner, (city#74 = city#44), rightHint=(strategy=broadcast)\n",
            "   :- Filter isnotnull(city#74)\n",
            "   :  +- LogicalRDD [userid#72, driverid#73, city#74, date#75, status#76, amount#77], false\n",
            "   +- Filter isnotnull(city#44)\n",
            "      +- LogicalRDD [city#44, region#45], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#74, userid#72, driverid#73, date#75, status#76, amount#77, region#45]\n",
            "   +- BroadcastHashJoin [city#74], [city#44], Inner, BuildRight, false\n",
            "      :- Filter isnotnull(city#74)\n",
            "      :  +- Scan ExistingRDD[userid#72,driverid#73,city#74,date#75,status#76,amount#77]\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=1415]\n",
            "         +- Filter isnotnull(city#44)\n",
            "            +- Scan ExistingRDD[city#44,region#45]\n",
            "\n",
            "+---------+------+--------+------------+---------+------+------+\n",
            "|     city|userid|driverid|        date|   status|amount|region|\n",
            "+---------+------+--------+------------+---------+------+------+\n",
            "|Hyderabad|  T001|    D001|  2024-01-05|Completed|   450| South|\n",
            "|Bangalore|  T002|    D002|  05/01/2024|Cancelled|     0| South|\n",
            "|   Mumbai|  T003|    D003|  2024/01/06|Completed|   620|  West|\n",
            "|    Delhi|  T004|    D004|invalid_date|Completed|   540| North|\n",
            "|Hyderabad|  T005|    D001|  2024-01-10|Completed|   700| South|\n",
            "|  Chennai|  T006|    D005|  2024-01-12|Completed|   350| South|\n",
            "+---------+------+--------+------------+---------+------+------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}