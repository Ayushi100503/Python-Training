{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vj9vphhBygq0"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "spark = SparkSession.builder \\\n",
        ".appName(\"digital learning platform\") \\\n",
        ".getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATASET 1 — USER REGISTRATION (CORRUPTED SCHEMA)"
      ],
      "metadata": {
        "id": "BaQeVyHQzBkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_users = [\n",
        "(\"U001\",\"Amit\",\"28\",\"Hyderabad\",\"['AI','ML','Cloud']\"),\n",
        "(\"U002\",\"Neha\",\"Thirty\",\"Delhi\",\"AI,Testing\"),\n",
        "(\"U003\",\"Ravi\",None,\"Bangalore\",[\"Data\",\"Spark\"]),\n",
        "(\"U004\",\"Pooja\",\"29\",\"Mumbai\",None),\n",
        "(\"U005\",\"\", \"31\",\"Chennai\",\"['DevOps']\")]"
      ],
      "metadata": {
        "id": "toj8C1QTy8Tv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *"
      ],
      "metadata": {
        "id": "icF1innHzP2M"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Design an explicit schema using StructType"
      ],
      "metadata": {
        "id": "NKnjPrzczLkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_schema = StructType([\n",
        "StructField(\"user_id\", StringType(), True),\n",
        "StructField(\"name\", StringType(), True),\n",
        "StructField(\"age_raw\", StringType(), True),\n",
        "StructField(\"city\", StringType(), True),\n",
        "StructField(\"skills_raw\", StringType(), True)])\n",
        "df_raw = spark.createDataFrame(raw_users, user_schema)\n",
        "df_raw.printSchema()\n",
        "df_raw.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waneFDqFzHdG",
        "outputId": "131a6758-6258-4615-e93d-0af7bfb5e8a3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age_raw: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- skills_raw: string (nullable = true)\n",
            "\n",
            "+-------+-----+-------+---------+-------------------+\n",
            "|user_id| name|age_raw|     city|         skills_raw|\n",
            "+-------+-----+-------+---------+-------------------+\n",
            "|   U001| Amit|     28|Hyderabad|['AI','ML','Cloud']|\n",
            "|   U002| Neha| Thirty|    Delhi|         AI,Testing|\n",
            "|   U003| Ravi|   NULL|Bangalore|      [Data, Spark]|\n",
            "|   U004|Pooja|     29|   Mumbai|               NULL|\n",
            "|   U005|     |     31|  Chennai|         ['DevOps']|\n",
            "+-------+-----+-------+---------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Normalize age into IntegerType"
      ],
      "metadata": {
        "id": "gWC7MJCWzxkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_age = df_raw.withColumn(\"age\", when(col(\"age_raw\").rlike(\"^[0-9]+$\"),\n",
        "col(\"age_raw\").cast(IntegerType())).otherwise(None))\n",
        "df_age.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpByVHtpzt7e",
        "outputId": "3b4f14ea-3d2d-4c39-b412-b25fbbf14d08"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+-------+---------+-------------------+----+\n",
            "|user_id| name|age_raw|     city|         skills_raw| age|\n",
            "+-------+-----+-------+---------+-------------------+----+\n",
            "|   U001| Amit|     28|Hyderabad|['AI','ML','Cloud']|  28|\n",
            "|   U002| Neha| Thirty|    Delhi|         AI,Testing|NULL|\n",
            "|   U003| Ravi|   NULL|Bangalore|      [Data, Spark]|NULL|\n",
            "|   U004|Pooja|     29|   Mumbai|               NULL|  29|\n",
            "|   U005|     |     31|  Chennai|         ['DevOps']|  31|\n",
            "+-------+-----+-------+---------+-------------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Normalize skills into ArrayType"
      ],
      "metadata": {
        "id": "BwlQmwu40lSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_skills = df_age.withColumn(\"skills\", when(col(\"skills_raw\").isNull(), array())\n",
        ".when(col(\"skills_raw\").startswith(\"[\"), split(regexp_replace(col(\"skills_raw\"), \"[\\\\[\\\\]']\", \"\"), \",\")).otherwise(split(col(\"skills_raw\"), \",\\\\s\")))\n",
        "df_skills.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLqnG_q60czg",
        "outputId": "23fee7df-858f-4321-828a-9b8e8c7c7a59"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+-------+---------+-------------------+----+---------------+\n",
            "|user_id| name|age_raw|     city|         skills_raw| age|         skills|\n",
            "+-------+-----+-------+---------+-------------------+----+---------------+\n",
            "|   U001| Amit|     28|Hyderabad|['AI','ML','Cloud']|  28|[AI, ML, Cloud]|\n",
            "|   U002| Neha| Thirty|    Delhi|         AI,Testing|NULL|   [AI,Testing]|\n",
            "|   U003| Ravi|   NULL|Bangalore|      [Data, Spark]|NULL| [Data,  Spark]|\n",
            "|   U004|Pooja|     29|   Mumbai|               NULL|  29|             []|\n",
            "|   U005|     |     31|  Chennai|         ['DevOps']|  31|       [DevOps]|\n",
            "+-------+-----+-------+---------+-------------------+----+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Handle empty or missing names"
      ],
      "metadata": {
        "id": "KdGQVpft2d-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_name = df_skills.withColumn(\"name\", when(col(\"name\").isNull() | (trim(col(\"name\")) == \"\"), lit(\"Unknown\")).otherwise(col(\"name\")))\n",
        "df_name.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F485aV032Y3f",
        "outputId": "a085798d-b5b8-41ec-af95-514a1023f0b8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-------+---------+-------------------+----+---------------+\n",
            "|user_id|   name|age_raw|     city|         skills_raw| age|         skills|\n",
            "+-------+-------+-------+---------+-------------------+----+---------------+\n",
            "|   U001|   Amit|     28|Hyderabad|['AI','ML','Cloud']|  28|[AI, ML, Cloud]|\n",
            "|   U002|   Neha| Thirty|    Delhi|         AI,Testing|NULL|   [AI,Testing]|\n",
            "|   U003|   Ravi|   NULL|Bangalore|      [Data, Spark]|NULL| [Data,  Spark]|\n",
            "|   U004|  Pooja|     29|   Mumbai|               NULL|  29|             []|\n",
            "|   U005|Unknown|     31|  Chennai|         ['DevOps']|  31|       [DevOps]|\n",
            "+-------+-------+-------+---------+-------------------+----+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Produce a clean users_df"
      ],
      "metadata": {
        "id": "_9pRt7vH3Qxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean_users = df_name.select(\"user_id\", \"name\", \"age\", \"city\", \"skills\")\n",
        "df_clean_users.show(truncate=False)\n",
        "df_clean_users.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZ_6VmXY3M0Y",
        "outputId": "22899b0d-7f7a-4b64-bc66-34394886223b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+----+---------+---------------+\n",
            "|user_id|name   |age |city     |skills         |\n",
            "+-------+-------+----+---------+---------------+\n",
            "|U001   |Amit   |28  |Hyderabad|[AI, ML, Cloud]|\n",
            "|U002   |Neha   |NULL|Delhi    |[AI,Testing]   |\n",
            "|U003   |Ravi   |NULL|Bangalore|[Data,  Spark] |\n",
            "|U004   |Pooja  |29  |Mumbai   |[]             |\n",
            "|U005   |Unknown|31  |Chennai  |[DevOps]       |\n",
            "+-------+-------+----+---------+---------------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- skills: array (nullable = true)\n",
            " |    |-- element: string (containsNull = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATASET 2 — COURSE CATALOG (NESTED STRUCT)"
      ],
      "metadata": {
        "id": "BOjB2JZK3nvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_courses = [\n",
        "(\"C001\",\"PySpark Mastery\",(\"Data Engineering\",\"Advanced\"),\"₹9999\"),\n",
        "(\"C002\",\"AI for Testers\",{\"domain\":\"QA\",\"level\":\"Beginner\"},\"8999\"),\n",
        "(\"C003\",\"ML Foundations\",(\"AI\",\"Intermediate\"),None),\n",
        "(\"C004\",\"Data Engineering Bootcamp\",\"Data|Advanced\",\"₹14999\")\n",
        "]"
      ],
      "metadata": {
        "id": "7u9kRI2W3l7T"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Create nested StructType for course metadata"
      ],
      "metadata": {
        "id": "FP5yvwBf4syQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "course_schema = StructType([\n",
        "    StructField(\"course_id\", StringType(), True),\n",
        "    StructField(\"course_name\", StringType(), True),\n",
        "    StructField(\"metadata_raw\", StringType(), True),\n",
        "    StructField(\"price_raw\", StringType(), True)\n",
        "])\n",
        "df_courses_raw = spark.createDataFrame(raw_courses, course_schema)\n",
        "df_courses_raw.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWFLPYds3kgu",
        "outputId": "fcbe9ebd-4068-4f5f-be3c-1e3df364669c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- course_id: string (nullable = true)\n",
            " |-- course_name: string (nullable = true)\n",
            " |-- metadata_raw: string (nullable = true)\n",
            " |-- price_raw: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Normalize domain and level"
      ],
      "metadata": {
        "id": "gSp8cBXp4vNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_courses_normalized = df_courses_raw.withColumn(\"domain\", \\\n",
        "    when(col(\"metadata_raw\").startswith(\"(\"), regexp_extract(col(\"metadata_raw\"), \"\\\\('([^']+)',\\\\s*'([^']+)'\\\\)\", 1)) \\\n",
        "    .when(col(\"metadata_raw\").startswith(\"{\"), regexp_extract(col(\"metadata_raw\"), \"'domain':\\\\s*'([^']+)'\", 1)) \\\n",
        "    .when(col(\"metadata_raw\").contains(\"|\"), split(col(\"metadata_raw\"), \"\\\\|\").getItem(0)) \\\n",
        "    .otherwise(lit(None))) \\\n",
        ".withColumn(\"level\", \\\n",
        "    when(col(\"metadata_raw\").startswith(\"(\"), regexp_extract(col(\"metadata_raw\"), \"\\\\('([^']+)',\\\\s*'([^']+)'\\\\)\", 2)) \\\n",
        "    .when(col(\"metadata_raw\").startswith(\"{\"), regexp_extract(col(\"metadata_raw\"), \"'level':\\\\s*'([^']+)'\", 1)) \\\n",
        "    .when(col(\"metadata_raw\").contains(\"|\"), split(col(\"metadata_raw\"), \"\\\\|\").getItem(1)) \\\n",
        "    .otherwise(lit(None)))\n",
        "\n",
        "df_courses_normalized.show(truncate=False)\n",
        "df_courses_normalized.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8y_TJpM24mv3",
        "outputId": "0ec78fbc-8dbe-4079-b3f1-88733f3fceb9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------------------+----------------------------+---------+------+--------+\n",
            "|course_id|course_name              |metadata_raw                |price_raw|domain|level   |\n",
            "+---------+-------------------------+----------------------------+---------+------+--------+\n",
            "|C001     |PySpark Mastery          |[Ljava.lang.Object;@6759b6c2|₹9999    |NULL  |NULL    |\n",
            "|C002     |AI for Testers           |{level=Beginner, domain=QA} |8999     |      |        |\n",
            "|C003     |ML Foundations           |[Ljava.lang.Object;@32db9d2b|NULL     |NULL  |NULL    |\n",
            "|C004     |Data Engineering Bootcamp|Data|Advanced               |₹14999   |Data  |Advanced|\n",
            "+---------+-------------------------+----------------------------+---------+------+--------+\n",
            "\n",
            "root\n",
            " |-- course_id: string (nullable = true)\n",
            " |-- course_name: string (nullable = true)\n",
            " |-- metadata_raw: string (nullable = true)\n",
            " |-- price_raw: string (nullable = true)\n",
            " |-- domain: string (nullable = true)\n",
            " |-- level: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Convert price to IntegerType"
      ],
      "metadata": {
        "id": "BkVRDoOZ5Iw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_price = df_courses_normalized.withColumn(\"price\", regexp_replace(col(\"price_raw\"), \"[^0-9]\", \"\").cast(IntegerType()))\n",
        "df_price.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHB2Zjxd5PNk",
        "outputId": "17c72c8d-3ea4-434d-e987-c471743fd624"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+--------------------+---------+------+--------+-----+\n",
            "|course_id|         course_name|        metadata_raw|price_raw|domain|   level|price|\n",
            "+---------+--------------------+--------------------+---------+------+--------+-----+\n",
            "|     C001|     PySpark Mastery|[Ljava.lang.Objec...|    ₹9999|  NULL|    NULL| 9999|\n",
            "|     C002|      AI for Testers|{level=Beginner, ...|     8999|      |        | 8999|\n",
            "|     C003|      ML Foundations|[Ljava.lang.Objec...|     NULL|  NULL|    NULL| NULL|\n",
            "|     C004|Data Engineering ...|       Data|Advanced|   ₹14999|  Data|Advanced|14999|\n",
            "+---------+--------------------+--------------------+---------+------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Handle missing prices"
      ],
      "metadata": {
        "id": "tb-mm3wJ68tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_price = df_price.fillna({\"price\":0})\n",
        "df_price.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43gIH63u66Wf",
        "outputId": "1c90336e-085f-400a-efa5-b01f1a25bbda"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+--------------------+---------+------+--------+-----+\n",
            "|course_id|         course_name|        metadata_raw|price_raw|domain|   level|price|\n",
            "+---------+--------------------+--------------------+---------+------+--------+-----+\n",
            "|     C001|     PySpark Mastery|[Ljava.lang.Objec...|    ₹9999|  NULL|    NULL| 9999|\n",
            "|     C002|      AI for Testers|{level=Beginner, ...|     8999|      |        | 8999|\n",
            "|     C003|      ML Foundations|[Ljava.lang.Objec...|     NULL|  NULL|    NULL|    0|\n",
            "|     C004|Data Engineering ...|       Data|Advanced|   ₹14999|  Data|Advanced|14999|\n",
            "+---------+--------------------+--------------------+---------+------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Produce courses_df"
      ],
      "metadata": {
        "id": "TCofdcYh7Quo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_courses_clean = df_price.select(\"course_id\", \"course_name\", \"domain\", \"level\", \"price\")\n",
        "df_courses_clean.show(truncate=False)\n",
        "df_courses_clean.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7hq7yTz7OaU",
        "outputId": "288c468d-322b-4c8a-dac0-848ab615e80c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------------------+------+--------+-----+\n",
            "|course_id|course_name              |domain|level   |price|\n",
            "+---------+-------------------------+------+--------+-----+\n",
            "|C001     |PySpark Mastery          |NULL  |NULL    |9999 |\n",
            "|C002     |AI for Testers           |      |        |8999 |\n",
            "|C003     |ML Foundations           |NULL  |NULL    |0    |\n",
            "|C004     |Data Engineering Bootcamp|Data  |Advanced|14999|\n",
            "+---------+-------------------------+------+--------+-----+\n",
            "\n",
            "root\n",
            " |-- course_id: string (nullable = true)\n",
            " |-- course_name: string (nullable = true)\n",
            " |-- domain: string (nullable = true)\n",
            " |-- level: string (nullable = true)\n",
            " |-- price: integer (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATASET 3 — USER COURSE ENROLLMENTS (JOIN + BROADCAST)"
      ],
      "metadata": {
        "id": "SwTF2-Pa7iBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_enrollments = [\n",
        "(\"U001\",\"C001\",\"2024-01-05\"),\n",
        "(\"U002\",\"C002\",\"05/01/2024\"),\n",
        "(\"U003\",\"C001\",\"2024/01/06\"),\n",
        "(\"U004\",\"C003\",\"invalid_date\"),\n",
        "(\"U001\",\"C004\",\"2024-01-10\")\n",
        "]"
      ],
      "metadata": {
        "id": "AbSpzZbn7xVb"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Normalize enrollment dates"
      ],
      "metadata": {
        "id": "ioDdfdUG7ogx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enroll_schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), True),\n",
        "    StructField(\"course_id\", StringType(), True),\n",
        "    StructField(\"enrollment_date_raw\", StringType(), True)\n",
        "])\n",
        "df_enrollments_raw = spark.createDataFrame(raw_enrollments, enroll_schema)\n",
        "df_enrollments_raw.printSchema()\n",
        "df_enrollments_raw.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw9eZveP7oGe",
        "outputId": "d1325315-265e-4a41-e494-2101349431ea"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- course_id: string (nullable = true)\n",
            " |-- enrollment_date_raw: string (nullable = true)\n",
            "\n",
            "+-------+---------+-------------------+\n",
            "|user_id|course_id|enrollment_date_raw|\n",
            "+-------+---------+-------------------+\n",
            "|   U001|     C001|         2024-01-05|\n",
            "|   U002|     C002|         05/01/2024|\n",
            "|   U003|     C001|         2024/01/06|\n",
            "|   U004|     C003|       invalid_date|\n",
            "|   U001|     C004|         2024-01-10|\n",
            "+-------+---------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Identify invalid enrollments"
      ],
      "metadata": {
        "id": "Nrd8ZiCO8LMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import coalesce, col, to_date, when\n",
        "\n",
        "df_enrollments_raw = df_enrollments_raw.withColumn(\n",
        "    \"enrollment_date\",\n",
        "    coalesce(\n",
        "        when(col(\"enrollment_date_raw\").rlike(\"^\\\\d{4}-\\\\d{2}-\\\\d{2}$\"), to_date(col(\"enrollment_date_raw\"), \"yyyy-MM-dd\")),\n",
        "        when(col(\"enrollment_date_raw\").rlike(\"^\\\\d{2}/\\\\d{2}/\\\\d{4}$\"), to_date(col(\"enrollment_date_raw\"), \"dd/MM/yyyy\")),\n",
        "        when(col(\"enrollment_date_raw\").rlike(\"^\\\\d{4}/\\\\d{2}/\\\\d{2}$\"), to_date(col(\"enrollment_date_raw\"), \"yyyy/MM/dd\"))\n",
        "    )\n",
        ")\n",
        "df_enrollments_raw.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Od9MZpAd8HUg",
        "outputId": "dc9fde92-47ba-488f-bf6e-05eb67300c69"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+-------------------+---------------+\n",
            "|user_id|course_id|enrollment_date_raw|enrollment_date|\n",
            "+-------+---------+-------------------+---------------+\n",
            "|   U001|     C001|         2024-01-05|     2024-01-05|\n",
            "|   U002|     C002|         05/01/2024|     2024-01-05|\n",
            "|   U003|     C001|         2024/01/06|     2024-01-06|\n",
            "|   U004|     C003|       invalid_date|           NULL|\n",
            "|   U001|     C004|         2024-01-10|     2024-01-10|\n",
            "+-------+---------+-------------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Join with users_df"
      ],
      "metadata": {
        "id": "wOV3mPrC85SQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_enrollments_processed = df_enrollments_raw.drop(\"enrollment_date_raw\")\n",
        "df_enriched = df_enrollments_processed.join(broadcast(df_courses_clean), on=\"course_id\", how=\"left\")\n",
        "df_enriched.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlD_jxrU8jHX",
        "outputId": "95f13ca6-ddb5-447f-db0c-80024df6325b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+---------------+--------------------+------+--------+-----+\n",
            "|course_id|user_id|enrollment_date|         course_name|domain|   level|price|\n",
            "+---------+-------+---------------+--------------------+------+--------+-----+\n",
            "|     C001|   U001|     2024-01-05|     PySpark Mastery|  NULL|    NULL| 9999|\n",
            "|     C002|   U002|     2024-01-05|      AI for Testers|      |        | 8999|\n",
            "|     C001|   U003|     2024-01-06|     PySpark Mastery|  NULL|    NULL| 9999|\n",
            "|     C003|   U004|           NULL|      ML Foundations|  NULL|    NULL|    0|\n",
            "|     C004|   U001|     2024-01-10|Data Engineering ...|  Data|Advanced|14999|\n",
            "+---------+-------+---------------+--------------------+------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Join with courses_df"
      ],
      "metadata": {
        "id": "mqPs_7op91ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_enriched.show(truncate=False)\n",
        "df_enriched.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guVgd5oc9ZG-",
        "outputId": "0eb8bd07-561d-465e-a6c4-a88607fd6c31"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+---------------+-------------------------+------+--------+-----+\n",
            "|course_id|user_id|enrollment_date|course_name              |domain|level   |price|\n",
            "+---------+-------+---------------+-------------------------+------+--------+-----+\n",
            "|C001     |U001   |2024-01-05     |PySpark Mastery          |NULL  |NULL    |9999 |\n",
            "|C002     |U002   |2024-01-05     |AI for Testers           |      |        |8999 |\n",
            "|C001     |U003   |2024-01-06     |PySpark Mastery          |NULL  |NULL    |9999 |\n",
            "|C003     |U004   |NULL           |ML Foundations           |NULL  |NULL    |0    |\n",
            "|C004     |U001   |2024-01-10     |Data Engineering Bootcamp|Data  |Advanced|14999|\n",
            "+---------+-------+---------------+-------------------------+------+--------+-----+\n",
            "\n",
            "root\n",
            " |-- course_id: string (nullable = true)\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- enrollment_date: date (nullable = true)\n",
            " |-- course_name: string (nullable = true)\n",
            " |-- domain: string (nullable = true)\n",
            " |-- level: string (nullable = true)\n",
            " |-- price: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Decide which table should be broadcast"
      ],
      "metadata": {
        "id": "3QD3CIa6-X1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision: Broadcast df_courses_clean\n",
        "# Reasoning: The `df_courses_clean` (course catalog) is expected to be significantly smaller than `df_enrollments_processed` (user enrollments).\n",
        "# Broadcasting the smaller table to all worker nodes during a join optimizes performance by avoiding a shuffle of the larger DataFrame and reducing network I/O.\n",
        "# This was already implemented in the previous join: `df_enrollments_processed.join(broadcast(df_courses_clean), on=\"course_id\", how=\"left\")`"
      ],
      "metadata": {
        "id": "fIY4ogcl-cEY"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Prove your choice using explain(True)"
      ],
      "metadata": {
        "id": "68TQQbWY-uNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_enriched.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1jZ5NIf-xLz",
        "outputId": "e2e1516d-5567-45d5-a8d2-d4eed4879d59"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(LeftOuter, [course_id])\n",
            ":- Project [user_id#190, course_id#191, enrollment_date#242]\n",
            ":  +- Project [user_id#190, course_id#191, enrollment_date_raw#192, coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy-MM-dd), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN to_date(enrollment_date_raw#192, Some(dd/MM/yyyy), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy/MM/dd), Some(Etc/UTC), true) END) AS enrollment_date#242]\n",
            ":     +- Project [user_id#190, course_id#191, enrollment_date_raw#192, coalesce(to_date(enrollment_date_raw#192, Some(yyyy-MM-dd), Some(Etc/UTC), true), to_date(enrollment_date_raw#192, Some(dd/MM/yyyy), Some(Etc/UTC), true), to_date(enrollment_date_raw#192, Some(yyyy/MM/dd), Some(Etc/UTC), true)) AS enrollment_date#203]\n",
            ":        +- LogicalRDD [user_id#190, course_id#191, enrollment_date_raw#192], false\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- Project [course_id#103, course_name#104, domain#107, level#108, price#151]\n",
            "      +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, level#108, coalesce(price#128, cast(0 as int)) AS price#151]\n",
            "         +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, level#108, cast(regexp_replace(price_raw#106, [^0-9], , 1) as int) AS price#128]\n",
            "            +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 2) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'level':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[1] ELSE cast(null as string) END AS level#108]\n",
            "               +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 1) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'domain':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[0] ELSE cast(null as string) END AS domain#107]\n",
            "                  +- LogicalRDD [course_id#103, course_name#104, metadata_raw#105, price_raw#106], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "course_id: string, user_id: string, enrollment_date: date, course_name: string, domain: string, level: string, price: int\n",
            "Project [course_id#191, user_id#190, enrollment_date#242, course_name#104, domain#107, level#108, price#151]\n",
            "+- Join LeftOuter, (course_id#191 = course_id#103)\n",
            "   :- Project [user_id#190, course_id#191, enrollment_date#242]\n",
            "   :  +- Project [user_id#190, course_id#191, enrollment_date_raw#192, coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy-MM-dd), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN to_date(enrollment_date_raw#192, Some(dd/MM/yyyy), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy/MM/dd), Some(Etc/UTC), true) END) AS enrollment_date#242]\n",
            "   :     +- Project [user_id#190, course_id#191, enrollment_date_raw#192, coalesce(to_date(enrollment_date_raw#192, Some(yyyy-MM-dd), Some(Etc/UTC), true), to_date(enrollment_date_raw#192, Some(dd/MM/yyyy), Some(Etc/UTC), true), to_date(enrollment_date_raw#192, Some(yyyy/MM/dd), Some(Etc/UTC), true)) AS enrollment_date#203]\n",
            "   :        +- LogicalRDD [user_id#190, course_id#191, enrollment_date_raw#192], false\n",
            "   +- ResolvedHint (strategy=broadcast)\n",
            "      +- Project [course_id#103, course_name#104, domain#107, level#108, price#151]\n",
            "         +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, level#108, coalesce(price#128, cast(0 as int)) AS price#151]\n",
            "            +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, level#108, cast(regexp_replace(price_raw#106, [^0-9], , 1) as int) AS price#128]\n",
            "               +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 2) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'level':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[1] ELSE cast(null as string) END AS level#108]\n",
            "                  +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 1) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'domain':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[0] ELSE cast(null as string) END AS domain#107]\n",
            "                     +- LogicalRDD [course_id#103, course_name#104, metadata_raw#105, price_raw#106], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [course_id#191, user_id#190, enrollment_date#242, course_name#104, domain#107, level#108, price#151]\n",
            "+- Join LeftOuter, (course_id#191 = course_id#103), rightHint=(strategy=broadcast)\n",
            "   :- Project [user_id#190, course_id#191, coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN cast(gettimestamp(enrollment_date_raw#192, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END) AS enrollment_date#242]\n",
            "   :  +- LogicalRDD [user_id#190, course_id#191, enrollment_date_raw#192], false\n",
            "   +- Project [course_id#103, course_name#104, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 1) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'domain':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[0] END AS domain#107, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 2) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'level':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[1] END AS level#108, coalesce(cast(regexp_replace(price_raw#106, [^0-9], , 1) as int), 0) AS price#151]\n",
            "      +- Filter isnotnull(course_id#103)\n",
            "         +- LogicalRDD [course_id#103, course_name#104, metadata_raw#105, price_raw#106], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [course_id#191, user_id#190, enrollment_date#242, course_name#104, domain#107, level#108, price#151]\n",
            "   +- BroadcastHashJoin [course_id#191], [course_id#103], LeftOuter, BuildRight, false\n",
            "      :- Project [user_id#190, course_id#191, coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN cast(gettimestamp(enrollment_date_raw#192, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END) AS enrollment_date#242]\n",
            "      :  +- Scan ExistingRDD[user_id#190,course_id#191,enrollment_date_raw#192]\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=442]\n",
            "         +- Project [course_id#103, course_name#104, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 1) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'domain':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[0] END AS domain#107, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 2) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'level':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[1] END AS level#108, coalesce(cast(regexp_replace(price_raw#106, [^0-9], , 1) as int), 0) AS price#151]\n",
            "            +- Filter isnotnull(course_id#103)\n",
            "               +- Scan ExistingRDD[course_id#103,course_name#104,metadata_raw#105,price_raw#106]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATASET 4 — USER ACTIVITY LOGS (ARRAY +\n",
        "MAP)"
      ],
      "metadata": {
        "id": "WN8Xt5OC_EsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_activity = [\n",
        "(\"U001\",\"login,watch,logout\",\"{'device':'mobile','ip':'1.1.1.1'}\",120),\n",
        "(\"U002\",[\"login\",\"watch\"],\"device=laptop;ip=2.2.2.2\",90),\n",
        "(\"U003\",\"login|logout\",None,30),\n",
        "(\"U004\",None,\"{'device':'tablet'}\",60)\n",
        "]"
      ],
      "metadata": {
        "id": "x2pACViL_Kyk"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Normalize actions into ArrayType"
      ],
      "metadata": {
        "id": "i58iC6qS_H_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import col, when, split\n",
        "\n",
        "# 1. Define the schema for the activity data\n",
        "activity_schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), True),\n",
        "    StructField(\"actions\", ArrayType(StringType()), True), # Normalize actions to ArrayType\n",
        "    StructField(\"properties\", StringType(), True),\n",
        "    StructField(\"duration\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# 2. Preprocess raw_activity to normalize the 'actions' field\n",
        "processed_raw_activity = []\n",
        "for user_id, actions_raw, properties, duration in raw_activity:\n",
        "    normalized_actions = None\n",
        "    if actions_raw is None:\n",
        "        normalized_actions = None\n",
        "    elif isinstance(actions_raw, list):\n",
        "        normalized_actions = actions_raw\n",
        "    elif isinstance(actions_raw, str):\n",
        "        if ',' in actions_raw:\n",
        "            normalized_actions = [a.strip() for a in actions_raw.split(',')]\n",
        "        elif '|' in actions_raw:\n",
        "            normalized_actions = [a.strip() for a in actions_raw.split('|')]\n",
        "\n",
        "    processed_raw_activity.append(Row(user_id=user_id, actions=normalized_actions, properties=properties, duration=duration))\n",
        "\n",
        "# 3. Create the DataFrame\n",
        "df_activity = spark.createDataFrame(processed_raw_activity, activity_schema)\n",
        "\n",
        "# Display the DataFrame and its schema\n",
        "df_activity.show(truncate=False)\n",
        "df_activity.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtRuphkzAipt",
        "outputId": "d848d247-7360-4906-d618-329eeab33b9f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------------+----------------------------------+--------+\n",
            "|user_id|actions               |properties                        |duration|\n",
            "+-------+----------------------+----------------------------------+--------+\n",
            "|U001   |[login, watch, logout]|{'device':'mobile','ip':'1.1.1.1'}|120     |\n",
            "|U002   |[login, watch]        |device=laptop;ip=2.2.2.2          |90      |\n",
            "|U003   |[login, logout]       |NULL                              |30      |\n",
            "|U004   |NULL                  |{'device':'tablet'}               |60      |\n",
            "+-------+----------------------+----------------------------------+--------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- actions: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- properties: string (nullable = true)\n",
            " |-- duration: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Normalize metadata into MapType"
      ],
      "metadata": {
        "id": "jf7-mODf_caM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, from_json, regexp_replace, udf\n",
        "from pyspark.sql.types import MapType, StringType\n",
        "\n",
        "# Define a UDF to parse custom key-value strings like \"device=laptop;ip=2.2.2.2\"\n",
        "def parse_custom_properties(s):\n",
        "    if s is None:\n",
        "        return None\n",
        "    try:\n",
        "        parts = s.split(';')\n",
        "        result_map = {}\n",
        "        for part in parts:\n",
        "            if '=' in part:\n",
        "                key, value = part.split('=', 1)\n",
        "                result_map[key.strip()] = value.strip()\n",
        "        return result_map\n",
        "    except Exception:\n",
        "        return None # Return None for malformed strings\n",
        "\n",
        "# Register the UDF\n",
        "parse_custom_properties_udf = udf(parse_custom_properties, MapType(StringType(), StringType()))\n",
        "\n",
        "df_activity_normalized = df_activity.withColumn(\n",
        "    \"properties\",\n",
        "    when(col(\"properties\").isNull(), None)\n",
        "    .when(col(\"properties\").startswith(\"{\"), # Check for JSON-like strings (start with '{')\n",
        "          from_json(regexp_replace(col(\"properties\"), \"'\", \"\\\"\"), MapType(StringType(), StringType())))\n",
        "    .otherwise(parse_custom_properties_udf(col(\"properties\"))) # Handle custom format for others\n",
        ")\n",
        "\n",
        "df_activity_normalized.show(truncate=False)\n",
        "df_activity_normalized.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxov4CI_Arbj",
        "outputId": "32f47acb-56e7-40e4-c1b5-b383567fcaff"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------------+---------------------------------+--------+\n",
            "|user_id|actions               |properties                       |duration|\n",
            "+-------+----------------------+---------------------------------+--------+\n",
            "|U001   |[login, watch, logout]|{device -> mobile, ip -> 1.1.1.1}|120     |\n",
            "|U002   |[login, watch]        |{device -> laptop, ip -> 2.2.2.2}|90      |\n",
            "|U003   |[login, logout]       |NULL                             |30      |\n",
            "|U004   |NULL                  |{device -> tablet}               |60      |\n",
            "+-------+----------------------+---------------------------------+--------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- actions: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            " |-- duration: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Handle missing actions safely"
      ],
      "metadata": {
        "id": "zZQUGbyC_g5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, array\n",
        "\n",
        "# Handle missing actions safely by replacing NULL with empty array\n",
        "df_activity_normalized = df_activity_normalized.withColumn(\n",
        "    \"actions\",\n",
        "    when(col(\"actions\").isNull(), array()).otherwise(col(\"actions\"))\n",
        ")\n",
        "\n",
        "df_activity_normalized.show(truncate=False)\n",
        "df_activity_normalized.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DX7jSFhLAyh4",
        "outputId": "68c07df8-77bd-445b-bd4d-d35fae4bf9c9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------------+---------------------------------+--------+\n",
            "|user_id|actions               |properties                       |duration|\n",
            "+-------+----------------------+---------------------------------+--------+\n",
            "|U001   |[login, watch, logout]|{device -> mobile, ip -> 1.1.1.1}|120     |\n",
            "|U002   |[login, watch]        |{device -> laptop, ip -> 2.2.2.2}|90      |\n",
            "|U003   |[login, logout]       |NULL                             |30      |\n",
            "|U004   |[]                    |{device -> tablet}               |60      |\n",
            "+-------+----------------------+---------------------------------+--------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- actions: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            " |-- duration: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Explode actions and count frequency"
      ],
      "metadata": {
        "id": "U5rVrsg6_jSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode, col, count\n",
        "\n",
        "# Explode the 'actions' array to create a new row for each action\n",
        "df_exploded_actions = df_activity_normalized.select(col(\"user_id\"), explode(col(\"actions\")).alias(\"action\"))\n",
        "\n",
        "# Count the frequency of each action\n",
        "action_frequency = df_exploded_actions.groupBy(\"action\").agg(count(\"action\").alias(\"frequency\"))\n",
        "\n",
        "# Show the results, ordered by frequency\n",
        "action_frequency.orderBy(col(\"frequency\").desc()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyZZfUyqA_xD",
        "outputId": "c539be93-8131-4227-bce6-8faa8d61d657"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+\n",
            "|action|frequency|\n",
            "+------+---------+\n",
            "| login|        3|\n",
            "| watch|        2|\n",
            "|logout|        2|\n",
            "+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Produce activity_df"
      ],
      "metadata": {
        "id": "wpUCmQ_C_k-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "activity_df = df_activity_normalized\n",
        "\n",
        "activity_df.show(truncate=False)\n",
        "activity_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnfBsZowBDT6",
        "outputId": "768a568e-47e2-4ea9-b82a-193b4e1f0118"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------------+---------------------------------+--------+\n",
            "|user_id|actions               |properties                       |duration|\n",
            "+-------+----------------------+---------------------------------+--------+\n",
            "|U001   |[login, watch, logout]|{device -> mobile, ip -> 1.1.1.1}|120     |\n",
            "|U002   |[login, watch]        |{device -> laptop, ip -> 2.2.2.2}|90      |\n",
            "|U003   |[login, logout]       |NULL                             |30      |\n",
            "|U004   |[]                    |{device -> tablet}               |60      |\n",
            "+-------+----------------------+---------------------------------+--------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- actions: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            " |-- duration: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATASET 5 — PAYMENTS (WINDOW + AGGREGATES)"
      ],
      "metadata": {
        "id": "Wu5P55VD_q2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_payments = [\n",
        "(\"U001\",\"2024-01-05\",9999),\n",
        "(\"U001\",\"2024-01-10\",14999),\n",
        "(\"U002\",\"2024-01-06\",8999),\n",
        "(\"U003\",\"2024-01-07\",0),\n",
        "(\"U004\",\"2024-01-08\",7999),\n",
        "(\"U001\",\"2024-01-15\",1999)\n",
        "]"
      ],
      "metadata": {
        "id": "svpAW4ak_xd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Convert dates properly"
      ],
      "metadata": {
        "id": "jV8eXO42CSmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date, col\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "# Define schema for raw_payments\n",
        "payment_schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), True),\n",
        "    StructField(\"payment_date_raw\", StringType(), True),\n",
        "    StructField(\"amount\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Create DataFrame from raw_payments\n",
        "df_payments = spark.createDataFrame(raw_payments, payment_schema)\n",
        "\n",
        "# Convert dates properly and drop the raw column\n",
        "df_payments = df_payments.withColumn(\"payment_date\", to_date(col(\"payment_date_raw\"), \"yyyy-MM-dd\")) \\\n",
        "                         .drop(\"payment_date_raw\")\n",
        "\n",
        "df_payments.printSchema()\n",
        "df_payments.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37jfNiO5BOcS",
        "outputId": "ab97ef36-16a9-4b9e-dbce-722601d426f1"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- amount: integer (nullable = true)\n",
            " |-- payment_date: date (nullable = true)\n",
            "\n",
            "+-------+------+------------+\n",
            "|user_id|amount|payment_date|\n",
            "+-------+------+------------+\n",
            "|   U001|  9999|  2024-01-05|\n",
            "|   U001| 14999|  2024-01-10|\n",
            "|   U002|  8999|  2024-01-06|\n",
            "|   U003|     0|  2024-01-07|\n",
            "|   U004|  7999|  2024-01-08|\n",
            "|   U001|  1999|  2024-01-15|\n",
            "+-------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Compute total spend per user (GroupBy)"
      ],
      "metadata": {
        "id": "N-hsb7LFDazF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_spend_per_user = df_payments.groupBy(\"user_id\").sum(\"amount\")\n",
        "total_spend_per_user.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRskyKjoCbCa",
        "outputId": "d103d94b-25d4-4ce3-bdfb-038749388508"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+\n",
            "|user_id|sum(amount)|\n",
            "+-------+-----------+\n",
            "|   U002|       8999|\n",
            "|   U001|      26997|\n",
            "|   U004|       7999|\n",
            "|   U003|          0|\n",
            "+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Compute running spend per user (Window)"
      ],
      "metadata": {
        "id": "xzlrnGKKDJSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "\n",
        "window_spec = Window.partitionBy(\"user_id\").orderBy(\"payment_date\")\n",
        "running_spend_per_user = df_payments.withColumn(\"running_spend\", sum(\"amount\").over(window_spec))\n",
        "running_spend_per_user.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HmiiINpDBWB",
        "outputId": "565791fc-4d7e-4b62-f1f7-e1bf0fe323f0"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+------------+-------------+\n",
            "|user_id|amount|payment_date|running_spend|\n",
            "+-------+------+------------+-------------+\n",
            "|   U001|  9999|  2024-01-05|         9999|\n",
            "|   U001| 14999|  2024-01-10|        24998|\n",
            "|   U001|  1999|  2024-01-15|        26997|\n",
            "|   U002|  8999|  2024-01-06|         8999|\n",
            "|   U003|     0|  2024-01-07|            0|\n",
            "|   U004|  7999|  2024-01-08|         7999|\n",
            "+-------+------+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Rank users by total spend"
      ],
      "metadata": {
        "id": "D3HeFU24Ds7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rank, desc\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "window_spec_rank = Window.orderBy(desc(\"sum(amount)\"))\n",
        "\n",
        "ranked_users_by_total_spend = total_spend_per_user.withColumn(\"rank\", rank().over(window_spec_rank))\n",
        "\n",
        "ranked_users_by_total_spend.show()\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anvLziZNDf1C",
        "outputId": "cb6100c6-2987-4865-9127-94f86eeef640"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+----+\n",
            "|user_id|sum(amount)|rank|\n",
            "+-------+-----------+----+\n",
            "|   U001|      26997|   1|\n",
            "|   U002|       8999|   2|\n",
            "|   U004|       7999|   3|\n",
            "|   U003|          0|   4|\n",
            "+-------+-----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Compare GroupBy vs Window outputs"
      ],
      "metadata": {
        "id": "naFQYUobDx3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total Spend Per User (GroupBy):\")\n",
        "total_spend_per_user.show()\n",
        "\n",
        "print(\"\\nRunning Spend Per User (Window Function):\")\n",
        "running_spend_per_user.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxEqu4rFECr0",
        "outputId": "5ca9ed11-b40b-435e-a3c0-a294978cddcf"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Spend Per User (GroupBy):\n",
            "+-------+-----------+\n",
            "|user_id|sum(amount)|\n",
            "+-------+-----------+\n",
            "|   U002|       8999|\n",
            "|   U001|      26997|\n",
            "|   U004|       7999|\n",
            "|   U003|          0|\n",
            "+-------+-----------+\n",
            "\n",
            "\n",
            "Running Spend Per User (Window Function):\n",
            "+-------+------+------------+-------------+\n",
            "|user_id|amount|payment_date|running_spend|\n",
            "+-------+------+------------+-------------+\n",
            "|   U001|  9999|  2024-01-05|         9999|\n",
            "|   U001| 14999|  2024-01-10|        24998|\n",
            "|   U001|  1999|  2024-01-15|        26997|\n",
            "|   U002|  8999|  2024-01-06|         8999|\n",
            "|   U003|     0|  2024-01-07|            0|\n",
            "|   U004|  7999|  2024-01-08|         7999|\n",
            "+-------+------+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATASET 6 — PARTITIONS & PERFORMANCE"
      ],
      "metadata": {
        "id": "E0gZ7FquEM5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Check default partitions for all DataFrames"
      ],
      "metadata": {
        "id": "4SHcSRDBEQPO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATASET 7 — DAG & OPTIMIZATION\n"
      ],
      "metadata": {
        "id": "OqmeTYbgESt6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. For each major transformation, run explain(True)"
      ],
      "metadata": {
        "id": "gIuwHCr0EXl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataframes_to_check = {\n",
        "    \"users_df\": df_clean_users,\n",
        "    \"courses_df\": df_courses_clean,\n",
        "    \"activity_df\": activity_df,\n",
        "    \"df_enrollments_processed\": df_enrollments_processed,\n",
        "    \"df_enriched\": df_enriched,\n",
        "    \"df_payments\": df_payments,\n",
        "    \"total_spend_per_user\": total_spend_per_user,\n",
        "    \"running_spend_per_user\": running_spend_per_user,\n",
        "    \"ranked_users_by_total_spend\": ranked_users_by_total_spend\n",
        "}\n",
        "\n",
        "print(\"--- Number of Partitions for DataFrames ---\")\n",
        "for df_name, df_obj in dataframes_to_check.items():\n",
        "    print(f\"DataFrame: {df_name}, Partitions: {df_obj.rdd.getNumPartitions()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uii46-V_Ebd3",
        "outputId": "ea7cc576-e75c-4939-b327-2465d4c454d4"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Number of Partitions for DataFrames ---\n",
            "DataFrame: users_df, Partitions: 2\n",
            "DataFrame: courses_df, Partitions: 2\n",
            "DataFrame: activity_df, Partitions: 2\n",
            "DataFrame: df_enrollments_processed, Partitions: 2\n",
            "DataFrame: df_enriched, Partitions: 2\n",
            "DataFrame: df_payments, Partitions: 2\n",
            "DataFrame: total_spend_per_user, Partitions: 1\n",
            "DataFrame: running_spend_per_user, Partitions: 1\n",
            "DataFrame: ranked_users_by_total_spend, Partitions: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Repartition enrollments by course_id"
      ],
      "metadata": {
        "id": "4FMlumzgEj-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repartitioned_enrollments = df_enrollments_processed.repartition(\"course_id\")\n",
        "print(f\"Original df_enrollments_processed partitions: {df_enrollments_processed.rdd.getNumPartitions()}\")\n",
        "print(f\"Repartitioned enrollments partitions: {repartitioned_enrollments.rdd.getNumPartitions()}\")\n",
        "repartitioned_enrollments.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yHH6qpHEixW",
        "outputId": "baf87346-908c-41b8-e561-58c9335c51b8"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original df_enrollments_processed partitions: 2\n",
            "Repartitioned enrollments partitions: 1\n",
            "+-------+---------+---------------+\n",
            "|user_id|course_id|enrollment_date|\n",
            "+-------+---------+---------------+\n",
            "|   U001|     C001|     2024-01-05|\n",
            "|   U002|     C002|     2024-01-05|\n",
            "|   U004|     C003|           NULL|\n",
            "|   U001|     C004|     2024-01-10|\n",
            "|   U003|     C001|     2024-01-06|\n",
            "+-------+---------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Coalesce results before writing"
      ],
      "metadata": {
        "id": "_0Xp4IRUEy00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coalesced_enrollments = repartitioned_enrollments.coalesce(1)\n",
        "print(f\"Repartitioned enrollments partitions: {repartitioned_enrollments.rdd.getNumPartitions()}\")\n",
        "print(f\"Coalesced enrollments partitions: {coalesced_enrollments.rdd.getNumPartitions()}\")\n",
        "coalesced_enrollments.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dco0MWlPE3Gn",
        "outputId": "b84a0e84-b96c-4bd7-89dc-e0289831c5f6"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repartitioned enrollments partitions: 1\n",
            "Coalesced enrollments partitions: 1\n",
            "+-------+---------+---------------+\n",
            "|user_id|course_id|enrollment_date|\n",
            "+-------+---------+---------------+\n",
            "|   U001|     C001|     2024-01-05|\n",
            "|   U002|     C002|     2024-01-05|\n",
            "|   U004|     C003|           NULL|\n",
            "|   U001|     C004|     2024-01-10|\n",
            "|   U003|     C001|     2024-01-06|\n",
            "+-------+---------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write outputs and inspect file counts"
      ],
      "metadata": {
        "id": "zU1J7ILVE2RG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "output_path = \"/tmp/coalesced_enrollments_output\"\n",
        "\n",
        "# Clean up previous runs if any\n",
        "if os.path.exists(output_path):\n",
        "    shutil.rmtree(output_path)\n",
        "\n",
        "# Write the coalesced DataFrame to a single Parquet file\n",
        "# Using mode(\"overwrite\") to handle re-runs smoothly\n",
        "coalesced_enrollments.write.mode(\"overwrite\").parquet(output_path)\n",
        "\n",
        "# Inspect file counts\n",
        "print(f\"Contents of {output_path}:\")\n",
        "files = os.listdir(output_path)\n",
        "for f in files:\n",
        "    print(f)\n",
        "\n",
        "# Count only the data files (e.g., .parquet files), excluding _SUCCESS and other metadata\n",
        "data_files = [f for f in files if f.endswith(\".parquet\")]\n",
        "print(f\"Number of data files written: {len(data_files)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrQJtq4IE_og",
        "outputId": "3d8f4189-8c64-450c-eda3-d908cfabebe4"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of /tmp/coalesced_enrollments_output:\n",
            ".part-00000-6c61c23d-5202-4a8d-8a56-93422c33d207-c000.snappy.parquet.crc\n",
            "._SUCCESS.crc\n",
            "_SUCCESS\n",
            "part-00000-6c61c23d-5202-4a8d-8a56-93422c33d207-c000.snappy.parquet\n",
            "Number of data files written: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Explain why repartition caused shuffle"
      ],
      "metadata": {
        "id": "ohSY9hikFEUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Repartitioning, especially by a key (like 'course_id' in our case), inherently involves a 'shuffle' operation in Spark.\n",
        "# A shuffle is a costly operation where data needs to be redistributed across the network among different executors or even within the same executor.\n",
        "# When you repartition by 'course_id', Spark needs to ensure that all rows with the same 'course_id' are moved to the same new partition.\n",
        "# To achieve this, it must read all the data, hash the 'course_id' for each row, and then send the row to the appropriate target partition.\n",
        "# This involves:\n",
        "# 1. Serialization: Converting data to a format that can be sent over the network.\n",
        "# 2. Network I/O: Transferring data between different nodes.\n",
        "# 3. Deserialization: Converting data back into an in-memory format on the receiving side.\n",
        "# 4. Disk I/O: Often, data is spilled to disk during shuffling if it doesn't fit in memory.\n",
        "# This is in contrast to transformations like `filter` or `select`, which are 'narrow transformations' and can often be performed on data within existing partitions without moving it."
      ],
      "metadata": {
        "id": "AAJBfO4gFFED"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATASET 7 — DAG & OPTIMIZATION"
      ],
      "metadata": {
        "id": "TJLZbUytFFkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. For each major transformation, run explain(True)"
      ],
      "metadata": {
        "id": "UxOoFsz-FSOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataframes_to_explain = {\n",
        "    \"df_clean_users\": df_clean_users,\n",
        "    \"df_courses_clean\": df_courses_clean,\n",
        "    \"activity_df\": activity_df,\n",
        "    \"df_enrollments_processed\": df_enrollments_processed,\n",
        "    \"df_enriched\": df_enriched,\n",
        "    \"df_payments\": df_payments,\n",
        "    \"total_spend_per_user\": total_spend_per_user,\n",
        "    \"running_spend_per_user\": running_spend_per_user,\n",
        "    \"ranked_users_by_total_spend\": ranked_users_by_total_spend\n",
        "}\n",
        "\n",
        "for df_name, df_obj in dataframes_to_explain.items():\n",
        "    print(f\"\\n--- Explain for DataFrame: {df_name} ---\")\n",
        "    df_obj.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tja9KrHTFMie",
        "outputId": "e9834ef3-16e7-459d-9602-29394b404b44"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Explain for DataFrame: df_clean_users ---\n",
            "== Parsed Logical Plan ==\n",
            "'Project ['user_id, 'name, 'age, 'city, 'skills]\n",
            "+- Project [user_id#0, CASE WHEN (isnull(name#1) OR (trim(name#1, None) = )) THEN Unknown ELSE name#1 END AS name#64, age_raw#2, city#3, skills_raw#4, age#21, skills#41]\n",
            "   +- Project [user_id#0, name#1, age_raw#2, city#3, skills_raw#4, age#21, CASE WHEN isnull(skills_raw#4) THEN cast(array() as array<string>) WHEN StartsWith(skills_raw#4, [) THEN split(regexp_replace(skills_raw#4, [\\[\\]'], , 1), ,, -1) ELSE split(skills_raw#4, ,\\s, -1) END AS skills#41]\n",
            "      +- Project [user_id#0, name#1, age_raw#2, city#3, skills_raw#4, CASE WHEN RLIKE(age_raw#2, ^[0-9]+$) THEN cast(age_raw#2 as int) ELSE cast(null as int) END AS age#21]\n",
            "         +- LogicalRDD [user_id#0, name#1, age_raw#2, city#3, skills_raw#4], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "user_id: string, name: string, age: int, city: string, skills: array<string>\n",
            "Project [user_id#0, name#64, age#21, city#3, skills#41]\n",
            "+- Project [user_id#0, CASE WHEN (isnull(name#1) OR (trim(name#1, None) = )) THEN Unknown ELSE name#1 END AS name#64, age_raw#2, city#3, skills_raw#4, age#21, skills#41]\n",
            "   +- Project [user_id#0, name#1, age_raw#2, city#3, skills_raw#4, age#21, CASE WHEN isnull(skills_raw#4) THEN cast(array() as array<string>) WHEN StartsWith(skills_raw#4, [) THEN split(regexp_replace(skills_raw#4, [\\[\\]'], , 1), ,, -1) ELSE split(skills_raw#4, ,\\s, -1) END AS skills#41]\n",
            "      +- Project [user_id#0, name#1, age_raw#2, city#3, skills_raw#4, CASE WHEN RLIKE(age_raw#2, ^[0-9]+$) THEN cast(age_raw#2 as int) ELSE cast(null as int) END AS age#21]\n",
            "         +- LogicalRDD [user_id#0, name#1, age_raw#2, city#3, skills_raw#4], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [user_id#0, CASE WHEN (isnull(name#1) OR (trim(name#1, None) = )) THEN Unknown ELSE name#1 END AS name#64, CASE WHEN RLIKE(age_raw#2, ^[0-9]+$) THEN cast(age_raw#2 as int) END AS age#21, city#3, CASE WHEN isnull(skills_raw#4) THEN [] WHEN StartsWith(skills_raw#4, [) THEN split(regexp_replace(skills_raw#4, [\\[\\]'], , 1), ,, -1) ELSE split(skills_raw#4, ,\\s, -1) END AS skills#41]\n",
            "+- LogicalRDD [user_id#0, name#1, age_raw#2, city#3, skills_raw#4], false\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [user_id#0, CASE WHEN (isnull(name#1) OR (trim(name#1, None) = )) THEN Unknown ELSE name#1 END AS name#64, CASE WHEN RLIKE(age_raw#2, ^[0-9]+$) THEN cast(age_raw#2 as int) END AS age#21, city#3, CASE WHEN isnull(skills_raw#4) THEN [] WHEN StartsWith(skills_raw#4, [) THEN split(regexp_replace(skills_raw#4, [\\[\\]'], , 1), ,, -1) ELSE split(skills_raw#4, ,\\s, -1) END AS skills#41]\n",
            "+- *(1) Scan ExistingRDD[user_id#0,name#1,age_raw#2,city#3,skills_raw#4]\n",
            "\n",
            "\n",
            "--- Explain for DataFrame: df_courses_clean ---\n",
            "== Parsed Logical Plan ==\n",
            "'Project ['course_id, 'course_name, 'domain, 'level, 'price]\n",
            "+- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, level#108, coalesce(price#128, cast(0 as int)) AS price#151]\n",
            "   +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, level#108, cast(regexp_replace(price_raw#106, [^0-9], , 1) as int) AS price#128]\n",
            "      +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 2) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'level':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[1] ELSE cast(null as string) END AS level#108]\n",
            "         +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 1) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'domain':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[0] ELSE cast(null as string) END AS domain#107]\n",
            "            +- LogicalRDD [course_id#103, course_name#104, metadata_raw#105, price_raw#106], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "course_id: string, course_name: string, domain: string, level: string, price: int\n",
            "Project [course_id#103, course_name#104, domain#107, level#108, price#151]\n",
            "+- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, level#108, coalesce(price#128, cast(0 as int)) AS price#151]\n",
            "   +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, level#108, cast(regexp_replace(price_raw#106, [^0-9], , 1) as int) AS price#128]\n",
            "      +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 2) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'level':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[1] ELSE cast(null as string) END AS level#108]\n",
            "         +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 1) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'domain':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[0] ELSE cast(null as string) END AS domain#107]\n",
            "            +- LogicalRDD [course_id#103, course_name#104, metadata_raw#105, price_raw#106], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [course_id#103, course_name#104, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 1) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'domain':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[0] END AS domain#107, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 2) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'level':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[1] END AS level#108, coalesce(cast(regexp_replace(price_raw#106, [^0-9], , 1) as int), 0) AS price#151]\n",
            "+- LogicalRDD [course_id#103, course_name#104, metadata_raw#105, price_raw#106], false\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [course_id#103, course_name#104, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 1) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'domain':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[0] END AS domain#107, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 2) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'level':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[1] END AS level#108, coalesce(cast(regexp_replace(price_raw#106, [^0-9], , 1) as int), 0) AS price#151]\n",
            "+- *(1) Scan ExistingRDD[course_id#103,course_name#104,metadata_raw#105,price_raw#106]\n",
            "\n",
            "\n",
            "--- Explain for DataFrame: activity_df ---\n",
            "== Parsed Logical Plan ==\n",
            "'Project [unresolvedstarwithcolumns(actions, CASE WHEN 'isNull('actions) THEN 'array() ELSE 'actions END, None)]\n",
            "+- Project [user_id#378, actions#379, CASE WHEN isnull(properties#380) THEN cast(null as map<string,string>) WHEN StartsWith(properties#380, {) THEN from_json(MapType(StringType,StringType,true), regexp_replace(properties#380, ', \", 1), Some(Etc/UTC), false) ELSE parse_custom_properties(properties#380)#395 END AS properties#396, duration#381]\n",
            "   +- LogicalRDD [user_id#378, actions#379, properties#380, duration#381], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "user_id: string, actions: array<string>, properties: map<string,string>, duration: int\n",
            "Project [user_id#378, CASE WHEN isnull(actions#379) THEN cast(array() as array<string>) ELSE actions#379 END AS actions#411, properties#396, duration#381]\n",
            "+- Project [user_id#378, actions#379, CASE WHEN isnull(properties#380) THEN cast(null as map<string,string>) WHEN StartsWith(properties#380, {) THEN from_json(MapType(StringType,StringType,true), regexp_replace(properties#380, ', \", 1), Some(Etc/UTC), false) ELSE parse_custom_properties(properties#380)#395 END AS properties#396, duration#381]\n",
            "   +- LogicalRDD [user_id#378, actions#379, properties#380, duration#381], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [user_id#378, CASE WHEN isnull(actions#379) THEN [] ELSE actions#379 END AS actions#411, CASE WHEN isnull(properties#380) THEN null WHEN StartsWith(properties#380, {) THEN from_json(MapType(StringType,StringType,true), regexp_replace(properties#380, ', \", 1), Some(Etc/UTC), false) ELSE pythonUDF0#543 END AS properties#396, duration#381]\n",
            "+- BatchEvalPython [parse_custom_properties(properties#380)#395], [pythonUDF0#543]\n",
            "   +- LogicalRDD [user_id#378, actions#379, properties#380, duration#381], false\n",
            "\n",
            "== Physical Plan ==\n",
            "Project [user_id#378, CASE WHEN isnull(actions#379) THEN [] ELSE actions#379 END AS actions#411, CASE WHEN isnull(properties#380) THEN null WHEN StartsWith(properties#380, {) THEN from_json(MapType(StringType,StringType,true), regexp_replace(properties#380, ', \", 1), Some(Etc/UTC), false) ELSE pythonUDF0#543 END AS properties#396, duration#381]\n",
            "+- BatchEvalPython [parse_custom_properties(properties#380)#395], [pythonUDF0#543]\n",
            "   +- *(1) Scan ExistingRDD[user_id#378,actions#379,properties#380,duration#381]\n",
            "\n",
            "\n",
            "--- Explain for DataFrame: df_enrollments_processed ---\n",
            "== Parsed Logical Plan ==\n",
            "Project [user_id#190, course_id#191, enrollment_date#242]\n",
            "+- Project [user_id#190, course_id#191, enrollment_date_raw#192, coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy-MM-dd), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN to_date(enrollment_date_raw#192, Some(dd/MM/yyyy), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy/MM/dd), Some(Etc/UTC), true) END) AS enrollment_date#242]\n",
            "   +- Project [user_id#190, course_id#191, enrollment_date_raw#192, coalesce(to_date(enrollment_date_raw#192, Some(yyyy-MM-dd), Some(Etc/UTC), true), to_date(enrollment_date_raw#192, Some(dd/MM/yyyy), Some(Etc/UTC), true), to_date(enrollment_date_raw#192, Some(yyyy/MM/dd), Some(Etc/UTC), true)) AS enrollment_date#203]\n",
            "      +- LogicalRDD [user_id#190, course_id#191, enrollment_date_raw#192], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "user_id: string, course_id: string, enrollment_date: date\n",
            "Project [user_id#190, course_id#191, enrollment_date#242]\n",
            "+- Project [user_id#190, course_id#191, enrollment_date_raw#192, coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy-MM-dd), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN to_date(enrollment_date_raw#192, Some(dd/MM/yyyy), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy/MM/dd), Some(Etc/UTC), true) END) AS enrollment_date#242]\n",
            "   +- Project [user_id#190, course_id#191, enrollment_date_raw#192, coalesce(to_date(enrollment_date_raw#192, Some(yyyy-MM-dd), Some(Etc/UTC), true), to_date(enrollment_date_raw#192, Some(dd/MM/yyyy), Some(Etc/UTC), true), to_date(enrollment_date_raw#192, Some(yyyy/MM/dd), Some(Etc/UTC), true)) AS enrollment_date#203]\n",
            "      +- LogicalRDD [user_id#190, course_id#191, enrollment_date_raw#192], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [user_id#190, course_id#191, coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN cast(gettimestamp(enrollment_date_raw#192, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END) AS enrollment_date#242]\n",
            "+- LogicalRDD [user_id#190, course_id#191, enrollment_date_raw#192], false\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [user_id#190, course_id#191, coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN cast(gettimestamp(enrollment_date_raw#192, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END) AS enrollment_date#242]\n",
            "+- *(1) Scan ExistingRDD[user_id#190,course_id#191,enrollment_date_raw#192]\n",
            "\n",
            "\n",
            "--- Explain for DataFrame: df_enriched ---\n",
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(LeftOuter, [course_id])\n",
            ":- Project [user_id#190, course_id#191, enrollment_date#242]\n",
            ":  +- Project [user_id#190, course_id#191, enrollment_date_raw#192, coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy-MM-dd), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN to_date(enrollment_date_raw#192, Some(dd/MM/yyyy), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy/MM/dd), Some(Etc/UTC), true) END) AS enrollment_date#242]\n",
            ":     +- Project [user_id#190, course_id#191, enrollment_date_raw#192, coalesce(to_date(enrollment_date_raw#192, Some(yyyy-MM-dd), Some(Etc/UTC), true), to_date(enrollment_date_raw#192, Some(dd/MM/yyyy), Some(Etc/UTC), true), to_date(enrollment_date_raw#192, Some(yyyy/MM/dd), Some(Etc/UTC), true)) AS enrollment_date#203]\n",
            ":        +- LogicalRDD [user_id#190, course_id#191, enrollment_date_raw#192], false\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- Project [course_id#103, course_name#104, domain#107, level#108, price#151]\n",
            "      +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, level#108, coalesce(price#128, cast(0 as int)) AS price#151]\n",
            "         +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, level#108, cast(regexp_replace(price_raw#106, [^0-9], , 1) as int) AS price#128]\n",
            "            +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 2) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'level':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[1] ELSE cast(null as string) END AS level#108]\n",
            "               +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 1) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'domain':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[0] ELSE cast(null as string) END AS domain#107]\n",
            "                  +- LogicalRDD [course_id#103, course_name#104, metadata_raw#105, price_raw#106], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "course_id: string, user_id: string, enrollment_date: date, course_name: string, domain: string, level: string, price: int\n",
            "Project [course_id#191, user_id#190, enrollment_date#242, course_name#104, domain#107, level#108, price#151]\n",
            "+- Join LeftOuter, (course_id#191 = course_id#103)\n",
            "   :- Project [user_id#190, course_id#191, enrollment_date#242]\n",
            "   :  +- Project [user_id#190, course_id#191, enrollment_date_raw#192, coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy-MM-dd), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN to_date(enrollment_date_raw#192, Some(dd/MM/yyyy), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy/MM/dd), Some(Etc/UTC), true) END) AS enrollment_date#242]\n",
            "   :     +- Project [user_id#190, course_id#191, enrollment_date_raw#192, coalesce(to_date(enrollment_date_raw#192, Some(yyyy-MM-dd), Some(Etc/UTC), true), to_date(enrollment_date_raw#192, Some(dd/MM/yyyy), Some(Etc/UTC), true), to_date(enrollment_date_raw#192, Some(yyyy/MM/dd), Some(Etc/UTC), true)) AS enrollment_date#203]\n",
            "   :        +- LogicalRDD [user_id#190, course_id#191, enrollment_date_raw#192], false\n",
            "   +- ResolvedHint (strategy=broadcast)\n",
            "      +- Project [course_id#103, course_name#104, domain#107, level#108, price#151]\n",
            "         +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, level#108, coalesce(price#128, cast(0 as int)) AS price#151]\n",
            "            +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, level#108, cast(regexp_replace(price_raw#106, [^0-9], , 1) as int) AS price#128]\n",
            "               +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, domain#107, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 2) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'level':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[1] ELSE cast(null as string) END AS level#108]\n",
            "                  +- Project [course_id#103, course_name#104, metadata_raw#105, price_raw#106, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 1) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'domain':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[0] ELSE cast(null as string) END AS domain#107]\n",
            "                     +- LogicalRDD [course_id#103, course_name#104, metadata_raw#105, price_raw#106], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [course_id#191, user_id#190, enrollment_date#242, course_name#104, domain#107, level#108, price#151]\n",
            "+- Join LeftOuter, (course_id#191 = course_id#103), rightHint=(strategy=broadcast)\n",
            "   :- Project [user_id#190, course_id#191, coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN cast(gettimestamp(enrollment_date_raw#192, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END) AS enrollment_date#242]\n",
            "   :  +- LogicalRDD [user_id#190, course_id#191, enrollment_date_raw#192], false\n",
            "   +- Project [course_id#103, course_name#104, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 1) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'domain':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[0] END AS domain#107, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 2) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'level':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[1] END AS level#108, coalesce(cast(regexp_replace(price_raw#106, [^0-9], , 1) as int), 0) AS price#151]\n",
            "      +- Filter isnotnull(course_id#103)\n",
            "         +- LogicalRDD [course_id#103, course_name#104, metadata_raw#105, price_raw#106], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=true\n",
            "+- == Final Plan ==\n",
            "   ResultQueryStage 1\n",
            "   +- *(2) Project [course_id#191, user_id#190, enrollment_date#242, course_name#104, domain#107, level#108, price#151]\n",
            "      +- *(2) BroadcastHashJoin [course_id#191], [course_id#103], LeftOuter, BuildRight, false\n",
            "         :- *(2) Project [user_id#190, course_id#191, coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN cast(gettimestamp(enrollment_date_raw#192, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END) AS enrollment_date#242]\n",
            "         :  +- *(2) Scan ExistingRDD[user_id#190,course_id#191,enrollment_date_raw#192]\n",
            "         +- BroadcastQueryStage 0\n",
            "            +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=1086]\n",
            "               +- *(1) Project [course_id#103, course_name#104, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 1) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'domain':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[0] END AS domain#107, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 2) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'level':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[1] END AS level#108, coalesce(cast(regexp_replace(price_raw#106, [^0-9], , 1) as int), 0) AS price#151]\n",
            "                  +- *(1) Filter isnotnull(course_id#103)\n",
            "                     +- *(1) Scan ExistingRDD[course_id#103,course_name#104,metadata_raw#105,price_raw#106]\n",
            "+- == Initial Plan ==\n",
            "   Project [course_id#191, user_id#190, enrollment_date#242, course_name#104, domain#107, level#108, price#151]\n",
            "   +- BroadcastHashJoin [course_id#191], [course_id#103], LeftOuter, BuildRight, false\n",
            "      :- Project [user_id#190, course_id#191, coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN cast(gettimestamp(enrollment_date_raw#192, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END) AS enrollment_date#242]\n",
            "      :  +- Scan ExistingRDD[user_id#190,course_id#191,enrollment_date_raw#192]\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=442]\n",
            "         +- Project [course_id#103, course_name#104, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 1) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'domain':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[0] END AS domain#107, CASE WHEN StartsWith(metadata_raw#105, () THEN regexp_extract(metadata_raw#105, \\('([^']+)',\\s*'([^']+)'\\), 2) WHEN StartsWith(metadata_raw#105, {) THEN regexp_extract(metadata_raw#105, 'level':\\s*'([^']+)', 1) WHEN Contains(metadata_raw#105, |) THEN split(metadata_raw#105, \\|, -1)[1] END AS level#108, coalesce(cast(regexp_replace(price_raw#106, [^0-9], , 1) as int), 0) AS price#151]\n",
            "            +- Filter isnotnull(course_id#103)\n",
            "               +- Scan ExistingRDD[course_id#103,course_name#104,metadata_raw#105,price_raw#106]\n",
            "\n",
            "\n",
            "--- Explain for DataFrame: df_payments ---\n",
            "== Parsed Logical Plan ==\n",
            "Project [user_id#456, amount#458, payment_date#459]\n",
            "+- Project [user_id#456, payment_date_raw#457, amount#458, to_date(payment_date_raw#457, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS payment_date#459]\n",
            "   +- LogicalRDD [user_id#456, payment_date_raw#457, amount#458], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "user_id: string, amount: int, payment_date: date\n",
            "Project [user_id#456, amount#458, payment_date#459]\n",
            "+- Project [user_id#456, payment_date_raw#457, amount#458, to_date(payment_date_raw#457, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS payment_date#459]\n",
            "   +- LogicalRDD [user_id#456, payment_date_raw#457, amount#458], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [user_id#456, amount#458, cast(gettimestamp(payment_date_raw#457, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) AS payment_date#459]\n",
            "+- LogicalRDD [user_id#456, payment_date_raw#457, amount#458], false\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [user_id#456, amount#458, cast(gettimestamp(payment_date_raw#457, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) AS payment_date#459]\n",
            "+- *(1) Scan ExistingRDD[user_id#456,payment_date_raw#457,amount#458]\n",
            "\n",
            "\n",
            "--- Explain for DataFrame: total_spend_per_user ---\n",
            "== Parsed Logical Plan ==\n",
            "'Aggregate ['user_id], ['user_id, unresolvedalias('sum(amount#458))]\n",
            "+- Project [user_id#456, amount#458, payment_date#459]\n",
            "   +- Project [user_id#456, payment_date_raw#457, amount#458, to_date(payment_date_raw#457, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS payment_date#459]\n",
            "      +- LogicalRDD [user_id#456, payment_date_raw#457, amount#458], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "user_id: string, sum(amount): bigint\n",
            "Aggregate [user_id#456], [user_id#456, sum(amount#458) AS sum(amount)#474L]\n",
            "+- Project [user_id#456, amount#458, payment_date#459]\n",
            "   +- Project [user_id#456, payment_date_raw#457, amount#458, to_date(payment_date_raw#457, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS payment_date#459]\n",
            "      +- LogicalRDD [user_id#456, payment_date_raw#457, amount#458], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Aggregate [user_id#456], [user_id#456, sum(amount#458) AS sum(amount)#474L]\n",
            "+- Project [user_id#456, amount#458]\n",
            "   +- LogicalRDD [user_id#456, payment_date_raw#457, amount#458], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=true\n",
            "+- == Final Plan ==\n",
            "   ResultQueryStage 1\n",
            "   +- *(2) HashAggregate(keys=[user_id#456], functions=[sum(amount#458)], output=[user_id#456, sum(amount)#474L])\n",
            "      +- AQEShuffleRead coalesced\n",
            "         +- ShuffleQueryStage 0\n",
            "            +- Exchange hashpartitioning(user_id#456, 200), ENSURE_REQUIREMENTS, [plan_id=1144]\n",
            "               +- *(1) HashAggregate(keys=[user_id#456], functions=[partial_sum(amount#458)], output=[user_id#456, sum#478L])\n",
            "                  +- *(1) Project [user_id#456, amount#458]\n",
            "                     +- *(1) Scan ExistingRDD[user_id#456,payment_date_raw#457,amount#458]\n",
            "+- == Initial Plan ==\n",
            "   HashAggregate(keys=[user_id#456], functions=[sum(amount#458)], output=[user_id#456, sum(amount)#474L])\n",
            "   +- Exchange hashpartitioning(user_id#456, 200), ENSURE_REQUIREMENTS, [plan_id=1133]\n",
            "      +- HashAggregate(keys=[user_id#456], functions=[partial_sum(amount#458)], output=[user_id#456, sum#478L])\n",
            "         +- Project [user_id#456, amount#458]\n",
            "            +- Scan ExistingRDD[user_id#456,payment_date_raw#457,amount#458]\n",
            "\n",
            "\n",
            "--- Explain for DataFrame: running_spend_per_user ---\n",
            "== Parsed Logical Plan ==\n",
            "'Project [unresolvedstarwithcolumns(running_spend, 'sum('amount) windowspecdefinition('user_id, 'payment_date ASC NULLS FIRST, unspecifiedframe$()), None)]\n",
            "+- Project [user_id#456, amount#458, payment_date#459]\n",
            "   +- Project [user_id#456, payment_date_raw#457, amount#458, to_date(payment_date_raw#457, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS payment_date#459]\n",
            "      +- LogicalRDD [user_id#456, payment_date_raw#457, amount#458], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "user_id: string, amount: int, payment_date: date, running_spend: bigint\n",
            "Project [user_id#456, amount#458, payment_date#459, running_spend#485L]\n",
            "+- Project [user_id#456, amount#458, payment_date#459, running_spend#485L, running_spend#485L]\n",
            "   +- Window [sum(amount#458) windowspecdefinition(user_id#456, payment_date#459 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS running_spend#485L], [user_id#456], [payment_date#459 ASC NULLS FIRST]\n",
            "      +- Project [user_id#456, amount#458, payment_date#459]\n",
            "         +- Project [user_id#456, amount#458, payment_date#459]\n",
            "            +- Project [user_id#456, payment_date_raw#457, amount#458, to_date(payment_date_raw#457, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS payment_date#459]\n",
            "               +- LogicalRDD [user_id#456, payment_date_raw#457, amount#458], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Window [sum(amount#458) windowspecdefinition(user_id#456, payment_date#459 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS running_spend#485L], [user_id#456], [payment_date#459 ASC NULLS FIRST]\n",
            "+- Project [user_id#456, amount#458, cast(gettimestamp(payment_date_raw#457, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) AS payment_date#459]\n",
            "   +- LogicalRDD [user_id#456, payment_date_raw#457, amount#458], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=true\n",
            "+- == Final Plan ==\n",
            "   ResultQueryStage 1\n",
            "   +- Window [sum(amount#458) windowspecdefinition(user_id#456, payment_date#459 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS running_spend#485L], [user_id#456], [payment_date#459 ASC NULLS FIRST]\n",
            "      +- *(2) Sort [user_id#456 ASC NULLS FIRST, payment_date#459 ASC NULLS FIRST], false, 0\n",
            "         +- AQEShuffleRead coalesced\n",
            "            +- ShuffleQueryStage 0\n",
            "               +- Exchange hashpartitioning(user_id#456, 200), ENSURE_REQUIREMENTS, [plan_id=1179]\n",
            "                  +- *(1) Project [user_id#456, amount#458, cast(gettimestamp(payment_date_raw#457, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) AS payment_date#459]\n",
            "                     +- *(1) Scan ExistingRDD[user_id#456,payment_date_raw#457,amount#458]\n",
            "+- == Initial Plan ==\n",
            "   Window [sum(amount#458) windowspecdefinition(user_id#456, payment_date#459 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS running_spend#485L], [user_id#456], [payment_date#459 ASC NULLS FIRST]\n",
            "   +- Sort [user_id#456 ASC NULLS FIRST, payment_date#459 ASC NULLS FIRST], false, 0\n",
            "      +- Exchange hashpartitioning(user_id#456, 200), ENSURE_REQUIREMENTS, [plan_id=1169]\n",
            "         +- Project [user_id#456, amount#458, cast(gettimestamp(payment_date_raw#457, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) AS payment_date#459]\n",
            "            +- Scan ExistingRDD[user_id#456,payment_date_raw#457,amount#458]\n",
            "\n",
            "\n",
            "--- Explain for DataFrame: ranked_users_by_total_spend ---\n",
            "== Parsed Logical Plan ==\n",
            "'Project [unresolvedstarwithcolumns(rank, 'rank() windowspecdefinition('sum(amount) DESC NULLS LAST, unspecifiedframe$()), None)]\n",
            "+- Aggregate [user_id#456], [user_id#456, sum(amount#458) AS sum(amount)#474L]\n",
            "   +- Project [user_id#456, amount#458, payment_date#459]\n",
            "      +- Project [user_id#456, payment_date_raw#457, amount#458, to_date(payment_date_raw#457, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS payment_date#459]\n",
            "         +- LogicalRDD [user_id#456, payment_date_raw#457, amount#458], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "user_id: string, sum(amount): bigint, rank: int\n",
            "Project [user_id#456, sum(amount)#474L, rank#501]\n",
            "+- Project [user_id#456, sum(amount)#474L, rank#501, rank#501]\n",
            "   +- Window [rank(sum(amount)#474L) windowspecdefinition(sum(amount)#474L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#501], [sum(amount)#474L DESC NULLS LAST]\n",
            "      +- Project [user_id#456, sum(amount)#474L]\n",
            "         +- Aggregate [user_id#456], [user_id#456, sum(amount#458) AS sum(amount)#474L]\n",
            "            +- Project [user_id#456, amount#458, payment_date#459]\n",
            "               +- Project [user_id#456, payment_date_raw#457, amount#458, to_date(payment_date_raw#457, Some(yyyy-MM-dd), Some(Etc/UTC), true) AS payment_date#459]\n",
            "                  +- LogicalRDD [user_id#456, payment_date_raw#457, amount#458], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Window [rank(sum(amount)#474L) windowspecdefinition(sum(amount)#474L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#501], [sum(amount)#474L DESC NULLS LAST]\n",
            "+- Aggregate [user_id#456], [user_id#456, sum(amount#458) AS sum(amount)#474L]\n",
            "   +- Project [user_id#456, amount#458]\n",
            "      +- LogicalRDD [user_id#456, payment_date_raw#457, amount#458], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=true\n",
            "+- == Final Plan ==\n",
            "   ResultQueryStage 2\n",
            "   +- Window [rank(sum(amount)#474L) windowspecdefinition(sum(amount)#474L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#501], [sum(amount)#474L DESC NULLS LAST]\n",
            "      +- *(3) Sort [sum(amount)#474L DESC NULLS LAST], false, 0\n",
            "         +- ShuffleQueryStage 1\n",
            "            +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=1262]\n",
            "               +- *(2) HashAggregate(keys=[user_id#456], functions=[sum(amount#458)], output=[user_id#456, sum(amount)#474L])\n",
            "                  +- AQEShuffleRead coalesced\n",
            "                     +- ShuffleQueryStage 0\n",
            "                        +- Exchange hashpartitioning(user_id#456, 200), ENSURE_REQUIREMENTS, [plan_id=1237]\n",
            "                           +- *(1) HashAggregate(keys=[user_id#456], functions=[partial_sum(amount#458)], output=[user_id#456, sum#478L])\n",
            "                              +- *(1) Project [user_id#456, amount#458]\n",
            "                                 +- *(1) Scan ExistingRDD[user_id#456,payment_date_raw#457,amount#458]\n",
            "+- == Initial Plan ==\n",
            "   Window [rank(sum(amount)#474L) windowspecdefinition(sum(amount)#474L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#501], [sum(amount)#474L DESC NULLS LAST]\n",
            "   +- Sort [sum(amount)#474L DESC NULLS LAST], false, 0\n",
            "      +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=1222]\n",
            "         +- HashAggregate(keys=[user_id#456], functions=[sum(amount#458)], output=[user_id#456, sum(amount)#474L])\n",
            "            +- Exchange hashpartitioning(user_id#456, 200), ENSURE_REQUIREMENTS, [plan_id=1219]\n",
            "               +- HashAggregate(keys=[user_id#456], functions=[partial_sum(amount#458)], output=[user_id#456, sum#478L])\n",
            "                  +- Project [user_id#456, amount#458]\n",
            "                     +- Scan ExistingRDD[user_id#456,payment_date_raw#457,amount#458]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Identify:\n",
        "Shuffles\n",
        "Sorts\n",
        "Broadcast joins"
      ],
      "metadata": {
        "id": "XRKNPXRgFXNn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "df_clean_users, df_courses_clean, df_enrollments_processed, df_payments: These DataFrames generally show straightforward Project and Scan ExistingRDD operations. There are no explicit shuffles, sorts, or broadcast joins at these initial transformation stages.\n",
        "\n",
        "activity_df: This DataFrame uses a Python UDF. While UDFs introduce their own overhead, the core Spark physical plan for this DataFrame doesn't show shuffles, sorts, or broadcast joins.\n",
        "\n",
        "df_enriched: This is where we clearly see a Broadcast Join! The physical plan contains BroadcastHashJoin (indicating the join strategy) and BroadcastExchange (confirming that df_courses_clean, the smaller table, was sent to all worker nodes).\n",
        "\n",
        "total_spend_per_user: This DataFrame involves a groupBy operation, which necessitates a Shuffle. You'll see Exchange hashpartitioning in its physical plan.\n",
        "\n",
        "running_spend_per_user: This DataFrame uses a window function with partitionBy and orderBy. This leads to both a Shuffle (Exchange hashpartitioning to group by user_id) and a Sort (Sort to order by payment_date within each user's partition).\n",
        "\n",
        "ranked_users_by_total_spend: This DataFrame is notable for multiple performance-impacting operations. It involves:\n",
        "\n",
        "Shuffles: An Exchange hashpartitioning for the initial aggregation (calculating total spend) and then critically, an Exchange SinglePartition to gather all aggregated data onto a single executor before global ranking.\n",
        "Sorts: A Sort operation is performed on this single partition to establish the global order required for the rank window function."
      ],
      "metadata": {
        "id": "xtDQK7RBHUCy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Identify one bad DAG"
      ],
      "metadata": {
        "id": "HDGxi8RuFudN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bad DAG identified in `ranked_users_by_total_spend`:\n",
        "# The physical plan for `ranked_users_by_total_spend` includes an `Exchange SinglePartition` followed by a global `Sort`.\n",
        "# This means that after computing the total spend per user (which already involves a shuffle for aggregation),\n",
        "# Spark then gathers ALL the aggregated data into a single partition (`Exchange SinglePartition`) to perform a global sort (`Sort`).\n",
        "# This design choice, while correct for achieving a global rank, is highly inefficient and becomes a major bottleneck\n",
        "# for large datasets as it eliminates parallelism and forces all data processing onto a single executor."
      ],
      "metadata": {
        "id": "WCwAmXYKFvJW"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Rewrite pipeline to improve it"
      ],
      "metadata": {
        "id": "tj0sYiPNF1Bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "good_df = df_enrollments_processed \\\n",
        ".filter(col(\"enrollment_date\").isNotNull())\\\n",
        ".join(broadcast(df_clean_users), \"user_id\")\\\n",
        ".groupBy(\"course_id\")\\\n",
        ".count()"
      ],
      "metadata": {
        "id": "0fGQIPibF4GJ"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Justify improvements using physical plan"
      ],
      "metadata": {
        "id": "aBJdQ1yAF7n4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "good_df.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scbgB-SnJGh0",
        "outputId": "90e32016-6959-4d13-c8e6-fbb70fad15a7"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Aggregate ['course_id], ['course_id, 'count(1) AS count#619]\n",
            "+- Project [user_id#190, course_id#191, enrollment_date#242, name#64, age#21, city#3, skills#41]\n",
            "   +- Join Inner, (user_id#190 = user_id#0)\n",
            "      :- Filter isnotnull(enrollment_date#242)\n",
            "      :  +- Project [user_id#190, course_id#191, enrollment_date#242]\n",
            "      :     +- Project [user_id#190, course_id#191, enrollment_date_raw#192, coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy-MM-dd), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN to_date(enrollment_date_raw#192, Some(dd/MM/yyyy), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy/MM/dd), Some(Etc/UTC), true) END) AS enrollment_date#242]\n",
            "      :        +- Project [user_id#190, course_id#191, enrollment_date_raw#192, coalesce(to_date(enrollment_date_raw#192, Some(yyyy-MM-dd), Some(Etc/UTC), true), to_date(enrollment_date_raw#192, Some(dd/MM/yyyy), Some(Etc/UTC), true), to_date(enrollment_date_raw#192, Some(yyyy/MM/dd), Some(Etc/UTC), true)) AS enrollment_date#203]\n",
            "      :           +- LogicalRDD [user_id#190, course_id#191, enrollment_date_raw#192], false\n",
            "      +- ResolvedHint (strategy=broadcast)\n",
            "         +- Project [user_id#0, name#64, age#21, city#3, skills#41]\n",
            "            +- Project [user_id#0, CASE WHEN (isnull(name#1) OR (trim(name#1, None) = )) THEN Unknown ELSE name#1 END AS name#64, age_raw#2, city#3, skills_raw#4, age#21, skills#41]\n",
            "               +- Project [user_id#0, name#1, age_raw#2, city#3, skills_raw#4, age#21, CASE WHEN isnull(skills_raw#4) THEN cast(array() as array<string>) WHEN StartsWith(skills_raw#4, [) THEN split(regexp_replace(skills_raw#4, [\\[\\]'], , 1), ,, -1) ELSE split(skills_raw#4, ,\\s, -1) END AS skills#41]\n",
            "                  +- Project [user_id#0, name#1, age_raw#2, city#3, skills_raw#4, CASE WHEN RLIKE(age_raw#2, ^[0-9]+$) THEN cast(age_raw#2 as int) ELSE cast(null as int) END AS age#21]\n",
            "                     +- LogicalRDD [user_id#0, name#1, age_raw#2, city#3, skills_raw#4], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "course_id: string, count: bigint\n",
            "Aggregate [course_id#191], [course_id#191, count(1) AS count#619L]\n",
            "+- Project [user_id#190, course_id#191, enrollment_date#242, name#64, age#21, city#3, skills#41]\n",
            "   +- Join Inner, (user_id#190 = user_id#0)\n",
            "      :- Filter isnotnull(enrollment_date#242)\n",
            "      :  +- Project [user_id#190, course_id#191, enrollment_date#242]\n",
            "      :     +- Project [user_id#190, course_id#191, enrollment_date_raw#192, coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy-MM-dd), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN to_date(enrollment_date_raw#192, Some(dd/MM/yyyy), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN to_date(enrollment_date_raw#192, Some(yyyy/MM/dd), Some(Etc/UTC), true) END) AS enrollment_date#242]\n",
            "      :        +- Project [user_id#190, course_id#191, enrollment_date_raw#192, coalesce(to_date(enrollment_date_raw#192, Some(yyyy-MM-dd), Some(Etc/UTC), true), to_date(enrollment_date_raw#192, Some(dd/MM/yyyy), Some(Etc/UTC), true), to_date(enrollment_date_raw#192, Some(yyyy/MM/dd), Some(Etc/UTC), true)) AS enrollment_date#203]\n",
            "      :           +- LogicalRDD [user_id#190, course_id#191, enrollment_date_raw#192], false\n",
            "      +- ResolvedHint (strategy=broadcast)\n",
            "         +- Project [user_id#0, name#64, age#21, city#3, skills#41]\n",
            "            +- Project [user_id#0, CASE WHEN (isnull(name#1) OR (trim(name#1, None) = )) THEN Unknown ELSE name#1 END AS name#64, age_raw#2, city#3, skills_raw#4, age#21, skills#41]\n",
            "               +- Project [user_id#0, name#1, age_raw#2, city#3, skills_raw#4, age#21, CASE WHEN isnull(skills_raw#4) THEN cast(array() as array<string>) WHEN StartsWith(skills_raw#4, [) THEN split(regexp_replace(skills_raw#4, [\\[\\]'], , 1), ,, -1) ELSE split(skills_raw#4, ,\\s, -1) END AS skills#41]\n",
            "                  +- Project [user_id#0, name#1, age_raw#2, city#3, skills_raw#4, CASE WHEN RLIKE(age_raw#2, ^[0-9]+$) THEN cast(age_raw#2 as int) ELSE cast(null as int) END AS age#21]\n",
            "                     +- LogicalRDD [user_id#0, name#1, age_raw#2, city#3, skills_raw#4], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Aggregate [course_id#191], [course_id#191, count(1) AS count#619L]\n",
            "+- Project [course_id#191]\n",
            "   +- Join Inner, (user_id#190 = user_id#0), rightHint=(strategy=broadcast)\n",
            "      :- Project [user_id#190, course_id#191]\n",
            "      :  +- Filter (isnotnull(coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN cast(gettimestamp(enrollment_date_raw#192, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END)) AND isnotnull(user_id#190))\n",
            "      :     +- LogicalRDD [user_id#190, course_id#191, enrollment_date_raw#192], false\n",
            "      +- Project [user_id#0]\n",
            "         +- Filter isnotnull(user_id#0)\n",
            "            +- LogicalRDD [user_id#0, name#1, age_raw#2, city#3, skills_raw#4], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[course_id#191], functions=[count(1)], output=[course_id#191, count#619L])\n",
            "   +- Exchange hashpartitioning(course_id#191, 200), ENSURE_REQUIREMENTS, [plan_id=1911]\n",
            "      +- HashAggregate(keys=[course_id#191], functions=[partial_count(1)], output=[course_id#191, count#629L])\n",
            "         +- Project [course_id#191]\n",
            "            +- BroadcastHashJoin [user_id#190], [user_id#0], Inner, BuildRight, false\n",
            "               :- Project [user_id#190, course_id#191]\n",
            "               :  +- Filter (isnotnull(coalesce(CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}-\\d{2}-\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{2}/\\d{2}/\\d{4}$) THEN cast(gettimestamp(enrollment_date_raw#192, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#192, ^\\d{4}/\\d{2}/\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#192, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END)) AND isnotnull(user_id#190))\n",
            "               :     +- Scan ExistingRDD[user_id#190,course_id#191,enrollment_date_raw#192]\n",
            "               +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=1906]\n",
            "                  +- Project [user_id#0]\n",
            "                     +- Filter isnotnull(user_id#0)\n",
            "                        +- Scan ExistingRDD[user_id#0,name#1,age_raw#2,city#3,skills_raw#4]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "StBetqCiIm5A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}